# PyArrow åœ¨ LeRobot ä¸­çš„ä½œç”¨

æœ¬æ–‡æ¡£è¯´æ˜ `pyarrow` åœ¨ LeRobot æ•°æ®é›†ä¸­çš„ä½œç”¨ï¼Œä»¥åŠå®ƒæ˜¯å¦ç”¨äºå…±äº«å†…å­˜ã€‚

## ğŸ“‹ æ ¸å¿ƒç­”æ¡ˆ

**PyArrow ä¸»è¦ç”¨äºå†…å­˜æ˜ å°„ï¼ˆMemory Mappingï¼‰ï¼Œè€Œä¸æ˜¯ä¼ ç»Ÿæ„ä¹‰ä¸Šçš„å…±äº«å†…å­˜ã€‚**

**ä¸»è¦ä½œç”¨**ï¼š
1. **è¯»å†™ Parquet æ–‡ä»¶**ï¼šå­˜å‚¨å’Œè¯»å–æ•°æ®é›†
2. **å†…å­˜æ˜ å°„**ï¼šé›¶æ‹·è´è®¿é—®ç£ç›˜æ•°æ®ï¼ˆä¸å ç”¨ RAMï¼‰
3. **ä¸ Hugging Face Datasets é›†æˆ**ï¼šæä¾›é«˜æ•ˆçš„æ•°æ®è®¿é—®

---

## ğŸ” PyArrow çš„ä¸»è¦ç”¨é€”

### 1. è¯»å†™ Parquet æ–‡ä»¶

**å†™å…¥æ•°æ®**ï¼š
```130:139:src/lerobot/datasets/lerobot_dataset.py
        table = pa.Table.from_pydict(combined_dict)

        if not self.writer:
            path = Path(self.root / DEFAULT_EPISODES_PATH.format(chunk_index=chunk_idx, file_index=file_idx))
            path.parent.mkdir(parents=True, exist_ok=True)
            self.writer = pq.ParquetWriter(
                path, schema=table.schema, compression="snappy", use_dictionary=True
            )

        self.writer.write_table(table)
```

**è¯»å–æ•°æ®**ï¼š
```125:127:src/lerobot/datasets/utils.py
def get_parquet_num_frames(parquet_path: str | Path) -> int:
    metadata = pq.read_metadata(parquet_path)
    return metadata.num_rows
```

### 2. å†…å­˜æ˜ å°„ï¼ˆMemory Mappingï¼‰

**å…³é”®æ³¨é‡Š**ï¼š
```332:333:src/lerobot/datasets/lerobot_dataset.py
        - `datasets` relies on a memory mapping from pyarrow (no RAM). It either converts parquet files to a pyarrow cache on disk,
          or loads directly from pyarrow cache.
```

**è¯´æ˜**ï¼š
- PyArrow ä½¿ç”¨**å†…å­˜æ˜ å°„**ï¼ˆmemory mappingï¼‰è®¿é—® Parquet æ–‡ä»¶
- **ä¸å ç”¨ RAM**ï¼šæ•°æ®ç›´æ¥ä»ç£ç›˜è¯»å–ï¼Œä¸éœ€è¦åŠ è½½åˆ°å†…å­˜
- **é›¶æ‹·è´**ï¼šå¤šä¸ªè¿›ç¨‹å¯ä»¥å…±äº«åŒä¸€ä»½ç£ç›˜æ•°æ®çš„å†…å­˜æ˜ å°„

### 3. ä¸ Hugging Face Datasets é›†æˆ

```106:122:src/lerobot/datasets/utils.py
def load_nested_dataset(pq_dir: Path, features: datasets.Features | None = None) -> Dataset:
    """Find parquet files in provided directory {pq_dir}/chunk-xxx/file-xxx.parquet
    Convert parquet files to pyarrow memory mapped in a cache folder for efficient RAM usage
    Concatenate all pyarrow references to return HF Dataset format

    Args:
        pq_dir: Directory containing parquet files
        features: Optional features schema to ensure consistent loading of complex types like images
    """
    paths = sorted(pq_dir.glob("*/*.parquet"))
    if len(paths) == 0:
        raise FileNotFoundError(f"Provided directory does not contain any parquet file: {pq_dir}")

    # TODO(rcadene): set num_proc to accelerate conversion to pyarrow
    with SuppressProgressBars():
        datasets = Dataset.from_parquet([str(path) for path in paths], features=features)
    return datasets
```

---

## ğŸ“Š å†…å­˜æ˜ å°„ vs å…±äº«å†…å­˜

### å†…å­˜æ˜ å°„ï¼ˆMemory Mappingï¼‰

**PyArrow ä½¿ç”¨çš„æ–¹å¼**ï¼š

| ç‰¹å¾ | å†…å­˜æ˜ å°„ |
|------|---------|
| **ç±»å‹** | æ–‡ä»¶æ˜ å°„åˆ°è™šæ‹Ÿå†…å­˜ |
| **å­˜å‚¨ä½ç½®** | ç£ç›˜æ–‡ä»¶ |
| **å†…å­˜å ç”¨** | ä¸å ç”¨ RAMï¼ˆæŒ‰éœ€åŠ è½½ï¼‰ |
| **è¿›ç¨‹å…±äº«** | âœ… å¤šä¸ªè¿›ç¨‹å¯ä»¥æ˜ å°„åŒä¸€æ–‡ä»¶ |
| **æŒä¹…åŒ–** | âœ… æ•°æ®æŒä¹…åŒ–åœ¨ç£ç›˜ |
| **é€Ÿåº¦** | è¾ƒå¿«ï¼ˆæ“ä½œç³»ç»Ÿç¼“å­˜ï¼‰ |

**å·¥ä½œåŸç†**ï¼š
```
ç£ç›˜ä¸Šçš„ Parquet æ–‡ä»¶
    â†“
æ“ä½œç³»ç»Ÿå†…å­˜æ˜ å°„
    â†“
è™šæ‹Ÿå†…å­˜åœ°å€ç©ºé—´
    â†“
æŒ‰éœ€åŠ è½½åˆ°ç‰©ç†å†…å­˜ï¼ˆæ“ä½œç³»ç»Ÿç®¡ç†ï¼‰
```

### å…±äº«å†…å­˜ï¼ˆShared Memoryï¼‰

**ä¼ ç»Ÿæ„ä¹‰ä¸Šçš„å…±äº«å†…å­˜**ï¼š

| ç‰¹å¾ | å…±äº«å†…å­˜ |
|------|---------|
| **ç±»å‹** | è¿›ç¨‹é—´å…±äº«å†…å­˜åŒºåŸŸ |
| **å­˜å‚¨ä½ç½®** | RAM |
| **å†…å­˜å ç”¨** | å ç”¨ RAM |
| **è¿›ç¨‹å…±äº«** | âœ… å¤šä¸ªè¿›ç¨‹å…±äº«åŒä¸€å†…å­˜åŒºåŸŸ |
| **æŒä¹…åŒ–** | âŒ è¿›ç¨‹ç»“æŸåæ¶ˆå¤± |
| **é€Ÿåº¦** | æœ€å¿«ï¼ˆç›´æ¥å†…å­˜è®¿é—®ï¼‰ |

**å·¥ä½œåŸç†**ï¼š
```
åˆ›å»ºå…±äº«å†…å­˜åŒºåŸŸ
    â†“
å¤šä¸ªè¿›ç¨‹æ˜ å°„åˆ°åŒä¸€å†…å­˜åœ°å€
    â†“
ç›´æ¥è¯»å†™å…±äº«å†…å­˜
```

---

## ğŸ¯ PyArrow åœ¨ LeRobot ä¸­çš„å…·ä½“ä½œç”¨

### 1. æ•°æ®å­˜å‚¨

**å†™å…¥ Parquet æ–‡ä»¶**ï¼š
```python
# åˆ›å»º PyArrow Table
table = pa.Table.from_pydict(combined_dict)

# å†™å…¥ Parquet æ–‡ä»¶
writer = pq.ParquetWriter(path, schema=table.schema)
writer.write_table(table)
```

### 2. æ•°æ®åŠ è½½

**ä» Parquet æ–‡ä»¶åŠ è½½**ï¼š
```python
# Hugging Face Datasets ä½¿ç”¨ PyArrow å†…å­˜æ˜ å°„åŠ è½½
dataset = Dataset.from_parquet([str(path) for path in paths])
```

**ä¼˜åŠ¿**ï¼š
- âœ… ä¸å ç”¨ RAMï¼šæ•°æ®ç›´æ¥ä»ç£ç›˜è¯»å–
- âœ… å¿«é€Ÿè®¿é—®ï¼šæ“ä½œç³»ç»Ÿç¼“å­˜å¸¸ç”¨æ•°æ®
- âœ… å¤šè¿›ç¨‹å‹å¥½ï¼šå¤šä¸ªè¿›ç¨‹å¯ä»¥æ˜ å°„åŒä¸€æ–‡ä»¶

### 3. é›¶æ‹·è´è®¿é—®

**å†…å­˜æ˜ å°„çš„ä¼˜åŠ¿**ï¼š
- å¤šä¸ªè¿›ç¨‹å¯ä»¥åŒæ—¶è®¿é—®åŒä¸€ Parquet æ–‡ä»¶
- ä¸éœ€è¦å¤åˆ¶æ•°æ®åˆ°å†…å­˜
- æ“ä½œç³»ç»Ÿè‡ªåŠ¨ç®¡ç†ç¼“å­˜

---

## ğŸ”§ æŠ€æœ¯ç»†èŠ‚

### PyArrow å†…å­˜æ˜ å°„çš„å·¥ä½œåŸç†

```
1. æ‰“å¼€ Parquet æ–‡ä»¶
   â†“
2. åˆ›å»ºå†…å­˜æ˜ å°„
   â†“
3. æ˜ å°„åˆ°è™šæ‹Ÿå†…å­˜åœ°å€ç©ºé—´
   â†“
4. æŒ‰éœ€åŠ è½½æ•°æ®é¡µï¼ˆpageï¼‰
   â†“
5. æ“ä½œç³»ç»Ÿç¼“å­˜å¸¸ç”¨æ•°æ®
```

### ä¸ Hugging Face Datasets çš„é›†æˆ

```1247:1248:src/lerobot/datasets/lerobot_dataset.py
        - `datasets` relies on a memory mapping from pyarrow (no RAM). It either converts parquet files to a pyarrow cache on disk,
          or loads directly from pyarrow cache.
```

**è¯´æ˜**ï¼š
- Hugging Face `datasets` åº“ä½¿ç”¨ PyArrow ä½œä¸ºåç«¯
- æ•°æ®ä»¥ PyArrow æ ¼å¼ç¼“å­˜åœ¨ç£ç›˜
- ä½¿ç”¨å†…å­˜æ˜ å°„è®¿é—®ï¼Œä¸å ç”¨ RAM

---

## ğŸ“Š å¯¹æ¯”æ€»ç»“

### PyArrow å†…å­˜æ˜ å°„ vs ä¼ ç»ŸåŠ è½½

| æ–¹å¼ | å†…å­˜å ç”¨ | åŠ è½½é€Ÿåº¦ | é€‚ç”¨åœºæ™¯ |
|------|---------|---------|---------|
| **PyArrow å†…å­˜æ˜ å°„** | ä½ï¼ˆæŒ‰éœ€ï¼‰ | å¿« | å¤§æ•°æ®é›† |
| **ä¼ ç»ŸåŠ è½½åˆ° RAM** | é«˜ï¼ˆå…¨éƒ¨ï¼‰ | æ…¢ | å°æ•°æ®é›† |

### PyArrow vs NumPy memmap

åœ¨ LeRobot ä¸­ï¼Œè¿˜æœ‰å¦ä¸€ä¸ªä½¿ç”¨å†…å­˜æ˜ å°„çš„åœ°æ–¹ï¼š

```19:22:src/lerobot/datasets/online_buffer.py
Note to maintainers: This duplicates some logic from LeRobotDataset and EpisodeAwareSampler. We should
consider converging to one approach. Here we have opted to use numpy.memmap to back the data buffer. It's much
faster than using HuggingFace Datasets as there's no conversion to an intermediate non-python object. Also it
supports in-place slicing and mutation which is very handy for a dynamic buffer.
```

**å¯¹æ¯”**ï¼š
- **PyArrow**ï¼šç”¨äº Parquet æ–‡ä»¶ï¼Œä¸ Hugging Face Datasets é›†æˆ
- **NumPy memmap**ï¼šç”¨äºåœ¨çº¿ç¼“å†²åŒºï¼Œæ”¯æŒåŸåœ°ä¿®æ”¹

---

## ğŸ’¡ ä¸ºä»€ä¹ˆä½¿ç”¨ PyArrowï¼Ÿ

### ä¼˜åŠ¿

1. **é«˜æ•ˆå­˜å‚¨**ï¼š
   - Parquet æ ¼å¼å‹ç¼©ç‡é«˜
   - åˆ—å¼å­˜å‚¨ï¼ŒæŸ¥è¯¢å¿«

2. **å†…å­˜æ•ˆç‡**ï¼š
   - å†…å­˜æ˜ å°„ï¼Œä¸å ç”¨ RAM
   - é€‚åˆå¤§æ•°æ®é›†

3. **æ ‡å‡†åŒ–**ï¼š
   - ä¸ Hugging Face ç”Ÿæ€ç³»ç»Ÿé›†æˆ
   - è·¨å¹³å°æ”¯æŒ

4. **å¤šè¿›ç¨‹å‹å¥½**ï¼š
   - å¤šä¸ªè¿›ç¨‹å¯ä»¥å…±äº«åŒä¸€æ–‡ä»¶çš„å†…å­˜æ˜ å°„
   - é€‚åˆåˆ†å¸ƒå¼è®­ç»ƒ

---

## ğŸ” ä»£ç ä¸­çš„ä½¿ç”¨

### 1. å†™å…¥å…ƒæ•°æ®

```111:142:src/lerobot/datasets/lerobot_dataset.py
    def _flush_metadata_buffer(self) -> None:
        """Write all buffered episode metadata to parquet file."""
        if not hasattr(self, "metadata_buffer") or len(self.metadata_buffer) == 0:
            return

        combined_dict = {}
        for episode_dict in self.metadata_buffer:
            for key, value in episode_dict.items():
                if key not in combined_dict:
                    combined_dict[key] = []
                # Extract value and serialize numpy arrays
                # because PyArrow's from_pydict function doesn't support numpy arrays
                val = value[0] if isinstance(value, list) else value
                combined_dict[key].append(val.tolist() if isinstance(val, np.ndarray) else val)

        first_ep = self.metadata_buffer[0]
        chunk_idx = first_ep["meta/episodes/chunk_index"][0]
        file_idx = first_ep["meta/episodes/file_index"][0]

        table = pa.Table.from_pydict(combined_dict)

        if not self.writer:
            path = Path(self.root / DEFAULT_EPISODES_PATH.format(chunk_index=chunk_idx, file_index=file_idx))
            path.parent.mkdir(parents=True, exist_ok=True)
            self.writer = pq.ParquetWriter(
                path, schema=table.schema, compression="snappy", use_dictionary=True
            )

        self.writer.write_table(table)

        self.latest_episode = self.metadata_buffer[-1]
        self.metadata_buffer.clear()
```

### 2. åŠ è½½æ•°æ®é›†

```106:122:src/lerobot/datasets/utils.py
def load_nested_dataset(pq_dir: Path, features: datasets.Features | None = None) -> Dataset:
    """Find parquet files in provided directory {pq_dir}/chunk-xxx/file-xxx.parquet
    Convert parquet files to pyarrow memory mapped in a cache folder for efficient RAM usage
    Concatenate all pyarrow references to return HF Dataset format

    Args:
        pq_dir: Directory containing parquet files
        features: Optional features schema to ensure consistent loading of complex types like images
    """
    paths = sorted(pq_dir.glob("*/*.parquet"))
    if len(paths) == 0:
        raise FileNotFoundError(f"Provided directory does not contain any parquet file: {pq_dir}")

    # TODO(rcadene): set num_proc to accelerate conversion to pyarrow
    with SuppressProgressBars():
        datasets = Dataset.from_parquet([str(path) for path in paths], features=features)
    return datasets
```

---

## ğŸ“ æ€»ç»“

### æ ¸å¿ƒç­”æ¡ˆ

**PyArrow ä¸»è¦ç”¨äºå†…å­˜æ˜ å°„ï¼ˆMemory Mappingï¼‰ï¼Œè€Œä¸æ˜¯ä¼ ç»Ÿæ„ä¹‰ä¸Šçš„å…±äº«å†…å­˜ã€‚**

### ä¸»è¦ä½œç”¨

1. **è¯»å†™ Parquet æ–‡ä»¶**ï¼šå­˜å‚¨å’Œè¯»å–æ•°æ®é›†
2. **å†…å­˜æ˜ å°„**ï¼šé›¶æ‹·è´è®¿é—®ç£ç›˜æ•°æ®
3. **å†…å­˜æ•ˆç‡**ï¼šä¸å ç”¨ RAMï¼Œé€‚åˆå¤§æ•°æ®é›†
4. **å¤šè¿›ç¨‹æ”¯æŒ**ï¼šå¤šä¸ªè¿›ç¨‹å¯ä»¥æ˜ å°„åŒä¸€æ–‡ä»¶

### å…³é”®åŒºåˆ«

| æ¦‚å¿µ | PyArrow ä½¿ç”¨ | ä¼ ç»Ÿå…±äº«å†…å­˜ |
|------|------------|------------|
| **ç±»å‹** | å†…å­˜æ˜ å°„æ–‡ä»¶ | å…±äº«å†…å­˜åŒºåŸŸ |
| **å­˜å‚¨** | ç£ç›˜æ–‡ä»¶ | RAM |
| **æŒä¹…åŒ–** | âœ… æ˜¯ | âŒ å¦ |
| **å†…å­˜å ç”¨** | ä½ï¼ˆæŒ‰éœ€ï¼‰ | é«˜ï¼ˆå…¨éƒ¨ï¼‰ |

### æŠ€æœ¯ä¼˜åŠ¿

- âœ… **é«˜æ•ˆ**ï¼šåˆ—å¼å­˜å‚¨ï¼Œå‹ç¼©ç‡é«˜
- âœ… **å†…å­˜å‹å¥½**ï¼šä¸å ç”¨ RAM
- âœ… **æ ‡å‡†åŒ–**ï¼šä¸ Hugging Face é›†æˆ
- âœ… **å¤šè¿›ç¨‹**ï¼šæ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒ

---

**PyArrow ä½¿ç”¨å†…å­˜æ˜ å°„å®ç°é«˜æ•ˆçš„æ•°æ®è®¿é—®ï¼Œè™½ç„¶ä¸æ˜¯ä¼ ç»Ÿæ„ä¹‰ä¸Šçš„å…±äº«å†…å­˜ï¼Œä½†æä¾›äº†ç±»ä¼¼çš„å¤šè¿›ç¨‹å…±äº«èƒ½åŠ›ï¼** ğŸ¯

