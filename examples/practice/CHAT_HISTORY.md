# LeRobot å­¦ä¹ å¯¹è¯å†å²è®°å½•

æœ¬æ–‡æ¡£è®°å½•äº†å­¦ä¹  LeRobot é¡¹ç›®è¿‡ç¨‹ä¸­çš„é‡è¦å¯¹è¯å’ŒçŸ¥è¯†ç‚¹ã€‚

---

## ğŸ“š ç›®å½•

1. [é¡¹ç›®ä¸»è¦æ¨¡å—è¯´æ˜](#é¡¹ç›®ä¸»è¦æ¨¡å—è¯´æ˜)
2. [æ•°æ®å½•åˆ¶æµç¨‹è¯¦è§£](#æ•°æ®å½•åˆ¶æµç¨‹è¯¦è§£)
3. [è®­ç»ƒæµç¨‹è¯¦è§£](#è®­ç»ƒæµç¨‹è¯¦è§£)
4. [å‡½æ•°è°ƒç”¨æµç¨‹](#å‡½æ•°è°ƒç”¨æµç¨‹)
5. [5ä¸ªæ­¥éª¤çš„è¯¦ç»†è§£é‡Š](#5ä¸ªæ­¥éª¤çš„è¯¦ç»†è§£é‡Š)
6. [å­¦ä¹ èµ„æº](#å­¦ä¹ èµ„æº)

---

## é¡¹ç›®ä¸»è¦æ¨¡å—è¯´æ˜

### æ ¸å¿ƒæ¨¡å—æ¶æ„

LeRobot é¡¹ç›®é‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼Œä¸»è¦åŒ…å«ä»¥ä¸‹æ ¸å¿ƒæ¨¡å—ï¼š

#### 1. **datasets/** - æ•°æ®é›†æ¨¡å—
- **åŠŸèƒ½**: æ•°æ®é›†ç®¡ç†ã€åŠ è½½ã€å¤„ç†å’Œä¸Šä¼ 
- **æ ¸å¿ƒç±»**: `LeRobotDataset`, `LeRobotDatasetMetadata`
- **å…³é”®åŠŸèƒ½**:
  - ä» Hugging Face Hub åŠ è½½æ•°æ®é›†
  - æœ¬åœ°æ•°æ®é›†ç®¡ç†
  - æ•°æ®é¢„å¤„ç†å’Œå¢å¼º
  - æ•°æ®é›†ä¸Šä¼ å’Œå…±äº«

#### 2. **policies/** - ç­–ç•¥æ¨¡å—
- **åŠŸèƒ½**: å„ç§æœºå™¨äººå­¦ä¹ ç­–ç•¥çš„å®ç°
- **æ”¯æŒçš„ç­–ç•¥**:
  - `act/` - ACT (Action Chunking with Transformers)
  - `diffusion/` - Diffusion Policy
  - `tdmpc/` - TD-MPC
  - `vqbet/` - VQ-BeT
  - `smolvla/` - SmolVLA
  - `groot/` - NVIDIA GR00T
  - `pi0/`, `pi05/` - Ï€â‚€ ç³»åˆ—
  - `sac/` - Soft Actor-Critic

#### 3. **robots/** - æœºå™¨äººæ¨¡å—
- **åŠŸèƒ½**: çœŸå®æœºå™¨äººçš„æ¥å£å’Œå®ç°
- **æ”¯æŒçš„æœºå™¨äºº**: SO-100/101, LeKiwi, Hope Jr, Koch, Reachy2 ç­‰

#### 4. **teleoperators/** - é¥æ“ä½œå™¨æ¨¡å—
- **åŠŸèƒ½**: ç”¨äºå½•åˆ¶æ¼”ç¤ºæ•°æ®çš„é¥æ“ä½œè®¾å¤‡
- **æ”¯æŒ**: SO-100/101 ä¸»åŠ¨è‡‚ã€æ‰‹æœºã€æ¸¸æˆæ‰‹æŸ„ã€é”®ç›˜ã€å¤–éª¨éª¼ç­‰

#### 5. **cameras/** - ç›¸æœºæ¨¡å—
- **åŠŸèƒ½**: ç›¸æœºæ¥å£å’Œå®ç°
- **æ”¯æŒ**: OpenCV, Intel RealSense, Reachy2 ç›¸æœº

#### 6. **processor/** - å¤„ç†å™¨æ¨¡å—
- **åŠŸèƒ½**: æ•°æ®å¤„ç†ç®¡é“ï¼Œè¿æ¥ä¸åŒç»„ä»¶
- **ä¸‰ä¸ªä¸»è¦ç®¡é“**:
  1. Teleop Action Processor: é¥æ“ä½œå™¨åŠ¨ä½œ â†’ æ•°æ®é›†åŠ¨ä½œ
  2. Robot Action Processor: æ•°æ®é›†åŠ¨ä½œ â†’ æœºå™¨äººå‘½ä»¤
  3. Robot Observation Processor: æœºå™¨äººè§‚å¯Ÿ â†’ æ•°æ®é›†è§‚å¯Ÿ

#### 7. **scripts/** - å‘½ä»¤è¡Œå·¥å…·
- **ä¸»è¦å·¥å…·**:
  - `lerobot-train` - è®­ç»ƒç­–ç•¥
  - `lerobot-record` - å½•åˆ¶æ•°æ®
  - `lerobot-eval` - è¯„ä¼°ç­–ç•¥
  - `lerobot-replay` - å›æ”¾æ•°æ®
  - `lerobot-dataset-viz` - å¯è§†åŒ–æ•°æ®é›†

---

## æ•°æ®å½•åˆ¶æµç¨‹è¯¦è§£

### æ•´ä½“æµç¨‹æ¦‚è§ˆ

```
Teleoperator â†’ Processor â†’ Robot â†’ Dataset
```

### è¯¦ç»†å‡½æ•°è°ƒç”¨é“¾

#### å…¥å£å‡½æ•°ï¼š`record()` (lerobot_record.py:372)

```python
@parser.wrap()
def record(cfg: RecordConfig) -> LeRobotDataset:
    # åˆå§‹åŒ–ç»„ä»¶
    robot = make_robot_from_config(cfg.robot)
    teleop = make_teleoperator_from_config(cfg.teleop)
    dataset = LeRobotDataset.create(...)
    
    # è¿æ¥è®¾å¤‡
    robot.connect()
    teleop.connect()
    
    # è¿›å…¥å½•åˆ¶å¾ªç¯
    record_loop(...)
```

#### æ ¸å¿ƒå¾ªç¯ï¼š`record_loop()` (lerobot_record.py:238)

ä¸»å¾ªç¯åœ¨ `record_loop()` ä¸­ï¼Œæ¯å¸§æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š

```python
def record_loop(
    robot: Robot,
    teleop: Teleoperator,
    dataset: LeRobotDataset,
    teleop_action_processor: RobotProcessorPipeline,
    robot_action_processor: RobotProcessorPipeline,
    robot_observation_processor: RobotProcessorPipeline,
    ...
):
    while timestamp < control_time_s:
        # === æ­¥éª¤ 1: è·å–æœºå™¨äººè§‚å¯Ÿ ===
        obs = robot.get_observation()  # è¡Œ 299
        
        # === æ­¥éª¤ 2: å¤„ç†è§‚å¯Ÿ ===
        obs_processed = robot_observation_processor(obs)  # è¡Œ 302
        
        # === æ­¥éª¤ 3: ä»é¥æ“ä½œå™¨è·å–åŠ¨ä½œ ===
        act = teleop.get_action()  # è¡Œ 323
        
        # === æ­¥éª¤ 4: å¤„ç†é¥æ“ä½œå™¨åŠ¨ä½œ ===
        act_processed_teleop = teleop_action_processor((act, obs))  # è¡Œ 326
        
        # === æ­¥éª¤ 5: å¤„ç†æœºå™¨äººåŠ¨ä½œ ===
        robot_action_to_send = robot_action_processor((act_processed_teleop, obs))  # è¡Œ 349
        
        # === æ­¥éª¤ 6: å‘é€åŠ¨ä½œåˆ°æœºå™¨äºº ===
        _sent_action = robot.send_action(robot_action_to_send)  # è¡Œ 355
        
        # === æ­¥éª¤ 7: ä¿å­˜åˆ°æ•°æ®é›† ===
        dataset.add_frame(frame)  # è¡Œ 361
```

---

## è®­ç»ƒæµç¨‹è¯¦è§£

### æ•´ä½“æµç¨‹æ¦‚è§ˆ

è®­ç»ƒæµç¨‹éµå¾ªä»¥ä¸‹å››ä¸ªä¸»è¦é˜¶æ®µï¼š

```
Dataset â†’ Processor â†’ Policy â†’ Training
```

### è¯¦ç»†æµç¨‹è¯´æ˜

#### 1. Datasetï¼ˆæ•°æ®é›†ï¼‰é˜¶æ®µ

**åŠŸèƒ½**ï¼šåŠ è½½å’Œå‡†å¤‡è®­ç»ƒæ•°æ®

**å…³é”®æ­¥éª¤**ï¼š

```183:190:src/lerobot/scripts/lerobot_train.py
    if is_main_process:
        logging.info("Creating dataset")
        dataset = make_dataset(cfg)

    accelerator.wait_for_everyone()

    # Now all other processes can safely load the dataset
    if not is_main_process:
        dataset = make_dataset(cfg)
```

**æ•°æ®é›†æä¾›çš„ä¿¡æ¯**ï¼š
- **å…ƒæ•°æ®ï¼ˆmetadataï¼‰**ï¼šåŒ…å«ç‰¹å¾å®šä¹‰ã€ç»Ÿè®¡ä¿¡æ¯ç­‰
- **ç»Ÿè®¡ä¿¡æ¯ï¼ˆstatsï¼‰**ï¼šç”¨äºå½’ä¸€åŒ–çš„å‡å€¼ã€æ ‡å‡†å·®ç­‰
- **ç‰¹å¾å®šä¹‰ï¼ˆfeaturesï¼‰**ï¼šè¾“å…¥è¾“å‡ºç‰¹å¾çš„å½¢çŠ¶å’Œç±»å‹

**å…³é”®æ•°æ®ç»“æ„**ï¼š
```python
dataset.meta.stats  # ç”¨äºå½’ä¸€åŒ–çš„ç»Ÿè®¡ä¿¡æ¯
dataset.meta.features  # ç‰¹å¾å®šä¹‰
```

---

#### 2. Processorï¼ˆå¤„ç†å™¨ï¼‰é˜¶æ®µ

**åŠŸèƒ½**ï¼šåˆ›å»ºæ•°æ®é¢„å¤„ç†å’Œåå¤„ç†ç®¡é“

**å…³é”®æ­¥éª¤**ï¼š

```212:244:src/lerobot/scripts/lerobot_train.py
    # Create processors - only provide dataset_stats if not resuming from saved processors
    processor_kwargs = {}
    postprocessor_kwargs = {}
    if (cfg.policy.pretrained_path and not cfg.resume) or not cfg.policy.pretrained_path:
        # Only provide dataset_stats when not resuming from saved processor state
        processor_kwargs["dataset_stats"] = dataset.meta.stats

    if cfg.policy.pretrained_path is not None:
        processor_kwargs["preprocessor_overrides"] = {
            "device_processor": {"device": device.type},
            "normalizer_processor": {
                "stats": dataset.meta.stats,
                "features": {**policy.config.input_features, **policy.config.output_features},
                "norm_map": policy.config.normalization_mapping,
            },
        }
        processor_kwargs["preprocessor_overrides"]["rename_observations_processor"] = {
            "rename_map": cfg.rename_map
        }
        postprocessor_kwargs["postprocessor_overrides"] = {
            "unnormalizer_processor": {
                "stats": dataset.meta.stats,
                "features": policy.config.output_features,
                "norm_map": policy.config.normalization_mapping,
            },
        }

    preprocessor, postprocessor = make_pre_post_processors(
        policy_cfg=cfg.policy,
        pretrained_path=cfg.policy.pretrained_path,
        **processor_kwargs,
        **postprocessor_kwargs,
    )
```

**Preprocessorï¼ˆé¢„å¤„ç†å™¨ï¼‰çš„ä½œç”¨**ï¼š

é¢„å¤„ç†å™¨å°†åŸå§‹æ•°æ®é›†æ‰¹æ¬¡è½¬æ¢ä¸ºæ¨¡å‹å¯æ¥å—çš„æ ¼å¼ï¼Œé€šå¸¸åŒ…æ‹¬ï¼š

1. **é‡å‘½åç‰¹å¾**ï¼šå°†æ•°æ®é›†ç‰¹å¾åæ˜ å°„åˆ°ç­–ç•¥æœŸæœ›çš„ç‰¹å¾å
2. **æ·»åŠ æ‰¹æ¬¡ç»´åº¦**ï¼šå°†å•æ ·æœ¬æ•°æ®è½¬æ¢ä¸ºæ‰¹æ¬¡æ ¼å¼
3. **è®¾å¤‡è½¬ç§»**ï¼šå°†æ•°æ®ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡ï¼ˆCPU/GPUï¼‰
4. **å½’ä¸€åŒ–**ï¼šä½¿ç”¨æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯å½’ä¸€åŒ–è¾“å…¥å’Œè¾“å‡ºç‰¹å¾

**ç¤ºä¾‹**ï¼ˆDiffusion Policy çš„é¢„å¤„ç†å™¨ï¼‰ï¼š

```65:74:src/lerobot/policies/diffusion/processor_diffusion.py
    input_steps = [
        RenameObservationsProcessorStep(rename_map={}),
        AddBatchDimensionProcessorStep(),
        DeviceProcessorStep(device=config.device),
        NormalizerProcessorStep(
            features={**config.input_features, **config.output_features},
            norm_map=config.normalization_mapping,
            stats=dataset_stats,
        ),
    ]
```

**Postprocessorï¼ˆåå¤„ç†å™¨ï¼‰çš„ä½œç”¨**ï¼š

åå¤„ç†å™¨å°†æ¨¡å‹è¾“å‡ºè½¬æ¢å›åŸå§‹å°ºåº¦ï¼Œé€šå¸¸åŒ…æ‹¬ï¼š

1. **åå½’ä¸€åŒ–**ï¼šå°†å½’ä¸€åŒ–çš„è¾“å‡ºè½¬æ¢å›åŸå§‹å°ºåº¦
2. **è®¾å¤‡è½¬ç§»**ï¼šå°†æ•°æ®ç§»å› CPU

**ç¤ºä¾‹**ï¼ˆDiffusion Policy çš„åå¤„ç†å™¨ï¼‰ï¼š

```75:80:src/lerobot/policies/diffusion/processor_diffusion.py
    output_steps = [
        UnnormalizerProcessorStep(
            features=config.output_features, norm_map=config.normalization_mapping, stats=dataset_stats
        ),
        DeviceProcessorStep(device="cpu"),
    ]
```

---

#### 3. Policyï¼ˆç­–ç•¥ï¼‰é˜¶æ®µ

**åŠŸèƒ½**ï¼šåˆ›å»ºå’Œåˆå§‹åŒ–ç­–ç•¥æ¨¡å‹

**å…³é”®æ­¥éª¤**ï¼š

```201:207:src/lerobot/scripts/lerobot_train.py
    if is_main_process:
        logging.info("Creating policy")
    policy = make_policy(
        cfg=cfg.policy,
        ds_meta=dataset.meta,
        rename_map=cfg.rename_map,
    )
```

**ç­–ç•¥åˆ›å»ºè¿‡ç¨‹**ï¼š

```339:427:src/lerobot/policies/factory.py
def make_policy(
    cfg: PreTrainedConfig,
    ds_meta: LeRobotDatasetMetadata | None = None,
    env_cfg: EnvConfig | None = None,
    rename_map: dict[str, str] | None = None,
) -> PreTrainedPolicy:
    """
    Instantiate a policy model.

    This factory function handles the logic of creating a policy, which requires
    determining the input and output feature shapes. These shapes can be derived
    either from a `LeRobotDatasetMetadata` object or an `EnvConfig` object. The function
    can either initialize a new policy from scratch or load a pretrained one.

    Args:
        cfg: The configuration for the policy to be created. If `cfg.pretrained_path` is
             set, the policy will be loaded with weights from that path.
        ds_meta: Dataset metadata used to infer feature shapes and types. Also provides
                 statistics for normalization layers.
        env_cfg: Environment configuration used to infer feature shapes and types.
                 One of `ds_meta` or `env_cfg` must be provided.
        rename_map: Optional mapping of dataset or environment feature keys to match
                 expected policy feature names (e.g., `"left"` â†’ `"camera1"`).

    Returns:
        An instantiated and device-placed policy model.

    Raises:
        ValueError: If both or neither of `ds_meta` and `env_cfg` are provided.
        NotImplementedError: If attempting to use an unsupported policy-backend
                             combination (e.g., VQBeT with 'mps').
    """
    if bool(ds_meta) == bool(env_cfg):
        raise ValueError("Either one of a dataset metadata or a sim env must be provided.")

    # NOTE: Currently, if you try to run vqbet with mps backend, you'll get this error.
    # TODO(aliberts, rcadene): Implement a check_backend_compatibility in policies?
    # NotImplementedError: The operator 'aten::unique_dim' is not currently implemented for the MPS device. If
    # you want this op to be added in priority during the prototype phase of this feature, please comment on
    # https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment
    # variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be
    # slower than running natively on MPS.
    if cfg.type == "vqbet" and cfg.device == "mps":
        raise NotImplementedError(
            "Current implementation of VQBeT does not support `mps` backend. "
            "Please use `cpu` or `cuda` backend."
        )

    policy_cls = get_policy_class(cfg.type)

    kwargs = {}
    if ds_meta is not None:
        features = dataset_to_policy_features(ds_meta.features)
    else:
        if not cfg.pretrained_path:
            logging.warning(
                "You are instantiating a policy from scratch and its features are parsed from an environment "
                "rather than a dataset. Normalization modules inside the policy will have infinite values "
                "by default without stats from a dataset."
            )
        if env_cfg is None:
            raise ValueError("env_cfg cannot be None when ds_meta is not provided")
        features = env_to_policy_features(env_cfg)

    if not cfg.output_features:
        cfg.output_features = {key: ft for key, ft in features.items() if ft.type is FeatureType.ACTION}
    if not cfg.input_features:
        cfg.input_features = {key: ft for key, ft in features.items() if key not in cfg.output_features}
    kwargs["config"] = cfg

    if cfg.pretrained_path:
        # Load a pretrained policy and override the config if needed (for example, if there are inference-time
        # hyperparameters that we want to vary).
        kwargs["pretrained_name_or_path"] = cfg.pretrained_path
        policy = policy_cls.from_pretrained(**kwargs)
    else:
        # Make a fresh policy.
        policy = policy_cls(**kwargs)

    policy.to(cfg.device)
    assert isinstance(policy, torch.nn.Module)

    # policy = torch.compile(policy, mode="reduce-overhead")

    if not rename_map:
        validate_visual_features_consistency(cfg, features)
        # TODO: (jadechoghari) - add a check_state(cfg, features) and check_action(cfg, features)

    return policy
```

**ç­–ç•¥çš„å…³é”®æ–¹æ³•**ï¼š

- **`forward(batch)`**ï¼šè®¡ç®—æŸå¤±ï¼Œç”¨äºè®­ç»ƒ
- **`select_action(batch)`**ï¼šé€‰æ‹©åŠ¨ä½œï¼Œç”¨äºæ¨ç†

**ç¤ºä¾‹**ï¼ˆDiffusion Policy çš„ forward æ–¹æ³•ï¼‰ï¼š

```140:147:src/lerobot/policies/diffusion/modeling_diffusion.py
    def forward(self, batch: dict[str, Tensor]) -> tuple[Tensor, None]:
        """Run the batch through the model and compute the loss for training or validation."""
        if self.config.image_features:
            batch = dict(batch)  # shallow copy so that adding a key doesn't modify the original
            batch[OBS_IMAGES] = torch.stack([batch[key] for key in self.config.image_features], dim=-4)
        loss = self.diffusion.compute_loss(batch)
        # no output_dict so returning None
        return loss, None
```

---

#### 4. Trainingï¼ˆè®­ç»ƒï¼‰é˜¶æ®µ

**åŠŸèƒ½**ï¼šæ‰§è¡Œè®­ç»ƒå¾ªç¯ï¼Œæ›´æ–°ç­–ç•¥å‚æ•°

**å…³é”®æ­¥éª¤**ï¼š

```326:340:src/lerobot/scripts/lerobot_train.py
    for _ in range(step, cfg.steps):
        start_time = time.perf_counter()
        batch = next(dl_iter)
        batch = preprocessor(batch)
        train_tracker.dataloading_s = time.perf_counter() - start_time

        train_tracker, output_dict = update_policy(
            train_tracker,
            policy,
            batch,
            optimizer,
            cfg.optimizer.grad_clip_norm,
            accelerator=accelerator,
            lr_scheduler=lr_scheduler,
        )
```

**è®­ç»ƒå¾ªç¯çš„è¯¦ç»†æµç¨‹**ï¼š

1. **è·å–æ‰¹æ¬¡**ï¼šä» DataLoader è·å–ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®
2. **é¢„å¤„ç†**ï¼šä½¿ç”¨ preprocessor å¤„ç†æ‰¹æ¬¡æ•°æ®
3. **å‰å‘ä¼ æ’­**ï¼šè°ƒç”¨ `policy.forward(batch)` è®¡ç®—æŸå¤±
4. **åå‘ä¼ æ’­**ï¼šè®¡ç®—æ¢¯åº¦
5. **ä¼˜åŒ–å™¨æ›´æ–°**ï¼šæ›´æ–°æ¨¡å‹å‚æ•°

**update_policy å‡½æ•°**ï¼š

```55:123:src/lerobot/scripts/lerobot_train.py
def update_policy(
    train_metrics: MetricsTracker,
    policy: PreTrainedPolicy,
    batch: Any,
    optimizer: Optimizer,
    grad_clip_norm: float,
    accelerator: Accelerator,
    lr_scheduler=None,
    lock=None,
) -> tuple[MetricsTracker, dict]:
    """
    Performs a single training step to update the policy's weights.

    This function executes the forward and backward passes, clips gradients, and steps the optimizer and
    learning rate scheduler. Accelerator handles mixed-precision training automatically.

    Args:
        train_metrics: A MetricsTracker instance to record training statistics.
        policy: The policy model to be trained.
        batch: A batch of training data.
        optimizer: The optimizer used to update the policy's parameters.
        grad_clip_norm: The maximum norm for gradient clipping.
        accelerator: The Accelerator instance for distributed training and mixed precision.
        lr_scheduler: An optional learning rate scheduler.
        lock: An optional lock for thread-safe optimizer updates.

    Returns:
        A tuple containing:
        - The updated MetricsTracker with new statistics for this step.
        - A dictionary of outputs from the policy's forward pass, for logging purposes.
    """
    start_time = time.perf_counter()
    policy.train()

    # Let accelerator handle mixed precision
    with accelerator.autocast():
        loss, output_dict = policy.forward(batch)
        # TODO(rcadene): policy.unnormalize_outputs(out_dict)

    # Use accelerator's backward method
    accelerator.backward(loss)

    # Clip gradients if specified
    if grad_clip_norm > 0:
        grad_norm = accelerator.clip_grad_norm_(policy.parameters(), grad_clip_norm)
    else:
        grad_norm = torch.nn.utils.clip_grad_norm_(
            policy.parameters(), float("inf"), error_if_nonfinite=False
        )

    # Optimizer step
    with lock if lock is not None else nullcontext():
        optimizer.step()

    optimizer.zero_grad()

    # Step through pytorch scheduler at every batch instead of epoch
    if lr_scheduler is not None:
        lr_scheduler.step()

    # Update internal buffers if policy has update method
    if has_method(accelerator.unwrap_model(policy, keep_fp32_wrapper=True), "update"):
        accelerator.unwrap_model(policy, keep_fp32_wrapper=True).update()

    train_metrics.loss = loss.item()
    train_metrics.grad_norm = grad_norm.item()
    train_metrics.lr = optimizer.param_groups[0]["lr"]
    train_metrics.update_s = time.perf_counter() - start_time
    return train_metrics, output_dict
```

---

### å®Œæ•´è®­ç»ƒæµç¨‹å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Dataset é˜¶æ®µ                                             â”‚
â”‚    â”œâ”€ make_dataset(cfg)                                     â”‚
â”‚    â”œâ”€ dataset.meta.stats  (ç»Ÿè®¡ä¿¡æ¯)                        â”‚
â”‚    â””â”€ dataset.meta.features  (ç‰¹å¾å®šä¹‰)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Processor é˜¶æ®µ                                           â”‚
â”‚    â”œâ”€ make_pre_post_processors()                            â”‚
â”‚    â”‚   â”œâ”€ preprocessor:                                     â”‚
â”‚    â”‚   â”‚   â”œâ”€ é‡å‘½åç‰¹å¾                                    â”‚
â”‚    â”‚   â”‚   â”œâ”€ æ·»åŠ æ‰¹æ¬¡ç»´åº¦                                  â”‚
â”‚    â”‚   â”‚   â”œâ”€ è®¾å¤‡è½¬ç§» (CPU â†’ GPU)                          â”‚
â”‚    â”‚   â”‚   â””â”€ å½’ä¸€åŒ– (ä½¿ç”¨ dataset.meta.stats)              â”‚
â”‚    â”‚   â””â”€ postprocessor:                                    â”‚
â”‚    â”‚       â”œâ”€ åå½’ä¸€åŒ–                                      â”‚
â”‚    â”‚       â””â”€ è®¾å¤‡è½¬ç§» (GPU â†’ CPU)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Policy é˜¶æ®µ                                              â”‚
â”‚    â”œâ”€ make_policy(cfg, ds_meta=dataset.meta)                â”‚
â”‚    â”‚   â”œâ”€ ä»æ•°æ®é›†å…ƒæ•°æ®æ¨æ–­è¾“å…¥/è¾“å‡ºç‰¹å¾                    â”‚
â”‚    â”‚   â”œâ”€ åˆ›å»ºç­–ç•¥æ¨¡å‹å®ä¾‹                                   â”‚
â”‚    â”‚   â””â”€ ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡                                     â”‚
â”‚    â””â”€ ç­–ç•¥æ–¹æ³•:                                             â”‚
â”‚        â”œâ”€ forward(batch) â†’ loss  (è®­ç»ƒ)                     â”‚
â”‚        â””â”€ select_action(batch) â†’ action  (æ¨ç†)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. Training é˜¶æ®µ                                            â”‚
â”‚                                                             â”‚
â”‚    for step in range(steps):                                â”‚
â”‚        [1] batch = next(dataloader)                         â”‚
â”‚        [2] batch = preprocessor(batch)  # å½’ä¸€åŒ–ã€è®¾å¤‡è½¬ç§»  â”‚
â”‚        [3] loss, output_dict = policy.forward(batch)        â”‚
â”‚        [4] loss.backward()  # åå‘ä¼ æ’­                      â”‚
â”‚        [5] optimizer.step()  # æ›´æ–°å‚æ•°                    â”‚
â”‚        [6] optimizer.zero_grad()                            â”‚
â”‚                                                             â”‚
â”‚    å®šæœŸæ“ä½œ:                                                â”‚
â”‚        - è®°å½•æŒ‡æ ‡ (log_freq)                                â”‚
â”‚        - ä¿å­˜æ£€æŸ¥ç‚¹ (save_freq)                             â”‚
â”‚        - è¯„ä¼°ç­–ç•¥ (eval_freq)                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### æ•°æ®æµè½¬ç¤ºä¾‹

**ç¤ºä¾‹ï¼šè®­ç»ƒ Diffusion Policy**

```python
# 1. Dataset é˜¶æ®µ
dataset = LeRobotDataset("lerobot/pusht")
# dataset.meta.stats åŒ…å«å½’ä¸€åŒ–æ‰€éœ€çš„å‡å€¼å’Œæ ‡å‡†å·®

# 2. Processor é˜¶æ®µ
preprocessor, postprocessor = make_pre_post_processors(
    cfg, 
    dataset_stats=dataset.meta.stats
)

# 3. Policy é˜¶æ®µ
policy = DiffusionPolicy(cfg)
policy.train()
policy.to(device)

# 4. Training é˜¶æ®µ
for batch in dataloader:
    # åŸå§‹æ‰¹æ¬¡: {"observation.image": tensor, "action": tensor, ...}
    batch = preprocessor(batch)
    # é¢„å¤„ç†å: å½’ä¸€åŒ–ã€æ·»åŠ æ‰¹æ¬¡ç»´åº¦ã€ç§»åŠ¨åˆ° GPU
    
    loss, _ = policy.forward(batch)
    # ç­–ç•¥è®¡ç®—æŸå¤±
    
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()
```

---

### å…³é”®æ¦‚å¿µæ€»ç»“

#### ä¸ºä»€ä¹ˆéœ€è¦ Processorï¼Ÿ

1. **æ•°æ®æ ¼å¼è½¬æ¢**ï¼šæ•°æ®é›†æ ¼å¼ â†’ æ¨¡å‹è¾“å…¥æ ¼å¼
2. **å½’ä¸€åŒ–**ï¼šä½¿ç”¨æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯å½’ä¸€åŒ–ï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§
3. **è®¾å¤‡ç®¡ç†**ï¼šè‡ªåŠ¨å¤„ç† CPU/GPU æ•°æ®è½¬ç§»
4. **æ‰¹æ¬¡å¤„ç†**ï¼šå°†å•æ ·æœ¬è½¬æ¢ä¸ºæ‰¹æ¬¡æ ¼å¼

#### ä¸ºä»€ä¹ˆéœ€è¦ Dataset Statsï¼Ÿ

- **å½’ä¸€åŒ–**ï¼šå°†ä¸åŒå°ºåº¦çš„ç‰¹å¾å½’ä¸€åŒ–åˆ°ç»Ÿä¸€èŒƒå›´
- **åå½’ä¸€åŒ–**ï¼šå°†æ¨¡å‹è¾“å‡ºè½¬æ¢å›åŸå§‹å°ºåº¦
- **è®­ç»ƒç¨³å®šæ€§**ï¼šå½’ä¸€åŒ–æœ‰åŠ©äºæ¢¯åº¦ç¨³å®šå’Œæ”¶æ•›

#### è®­ç»ƒå¾ªç¯çš„å…³é”®æ­¥éª¤

1. **æ•°æ®åŠ è½½**ï¼šä» DataLoader è·å–æ‰¹æ¬¡
2. **é¢„å¤„ç†**ï¼šä½¿ç”¨ preprocessor å¤„ç†æ•°æ®
3. **å‰å‘ä¼ æ’­**ï¼šè®¡ç®—æŸå¤±
4. **åå‘ä¼ æ’­**ï¼šè®¡ç®—æ¢¯åº¦
5. **å‚æ•°æ›´æ–°**ï¼šä¼˜åŒ–å™¨æ›´æ–°æ¨¡å‹å‚æ•°

---

## å‡½æ•°è°ƒç”¨æµç¨‹

### å®Œæ•´è°ƒç”¨æµç¨‹å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ record() - ä¸»å…¥å£å‡½æ•°                                        â”‚
â”‚  â”œâ”€ make_robot_from_config()                                â”‚
â”‚  â”œâ”€ make_teleoperator_from_config()                          â”‚
â”‚  â”œâ”€ LeRobotDataset.create()                                 â”‚
â”‚  â”œâ”€ robot.connect()                                          â”‚
â”‚  â”œâ”€ teleop.connect()                                         â”‚
â”‚  â””â”€ record_loop() â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                                                                â”‚
                                                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ record_loop() - ä¸»å¾ªç¯ (æ¯å¸§æ‰§è¡Œ)                            â”‚
â”‚                                                              â”‚
â”‚  while timestamp < control_time_s:                          â”‚
â”‚                                                              â”‚
â”‚    [1] obs = robot.get_observation()                        â”‚
â”‚        â”œâ”€ bus.sync_read("Present_Position")                 â”‚
â”‚        â””â”€ cam.async_read()                                   â”‚
â”‚                                                              â”‚
â”‚    [2] obs_processed = robot_observation_processor(obs)    â”‚
â”‚        â””â”€ å¯èƒ½åŒ…æ‹¬å½’ä¸€åŒ–ã€é‡å‘½åç­‰                           â”‚
â”‚                                                              â”‚
â”‚    [3] act = teleop.get_action()                            â”‚
â”‚        â””â”€ bus.read("Present_Position")                      â”‚
â”‚                                                              â”‚
â”‚    [4] act_processed = teleop_action_processor((act, obs)) â”‚
â”‚        â””â”€ è½¬æ¢ä¸ºæ•°æ®é›†æ ¼å¼                                   â”‚
â”‚                                                              â”‚
â”‚    [5] robot_action = robot_action_processor(...)           â”‚
â”‚        â””â”€ è½¬æ¢ä¸ºæœºå™¨äººå‘½ä»¤æ ¼å¼                               â”‚
â”‚                                                              â”‚
â”‚    [6] robot.send_action(robot_action)                       â”‚
â”‚        â”œâ”€ ensure_safe_goal_position()                       â”‚
â”‚        â””â”€ bus.sync_write("Goal_Position", goal_pos)         â”‚
â”‚                                                              â”‚
â”‚    [7] dataset.add_frame(frame)                             â”‚
â”‚        â”œâ”€ validate_frame()                                   â”‚
â”‚        â”œâ”€ _save_image() (å›¾åƒå†™å…¥æ–‡ä»¶)                       â”‚
â”‚        â””â”€ episode_buffer[key].append() (å…¶ä»–æ•°æ®)            â”‚
â”‚                                                              â”‚
â”‚    [8] busy_wait(1/fps - dt)  # æ§åˆ¶å¸§ç‡                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 5ä¸ªæ­¥éª¤çš„è¯¦ç»†è§£é‡Š

### æ ¸å¿ƒé—®é¢˜ï¼šä¸ºä»€ä¹ˆéœ€è¦è¿™äº›æ­¥éª¤ï¼Ÿ

ä¸åŒç»„ä»¶ä½¿ç”¨ä¸åŒçš„æ•°æ®è¡¨ç¤ºï¼š
- **é¥æ“ä½œå™¨**ï¼šå¯èƒ½æ˜¯å…³èŠ‚ä½ç½®ã€æœ«ç«¯æ‰§è¡Œå™¨å¢é‡ã€æ‰‹æœºå§¿æ€ç­‰
- **æ•°æ®é›†**ï¼šéœ€è¦ç»Ÿä¸€ã€æ ‡å‡†åŒ–çš„æ ¼å¼ï¼ˆå¦‚å½’ä¸€åŒ–çš„æœ«ç«¯æ‰§è¡Œå™¨ä½ç½®ï¼‰
- **æœºå™¨äºº**ï¼šéœ€è¦å…³èŠ‚ç›®æ ‡ä½ç½®æˆ–ç”µæœºå‘½ä»¤

å› æ­¤éœ€è¦å¤„ç†å™¨ç®¡é“è¿›è¡Œè½¬æ¢ã€‚

---

### æ­¥éª¤ 1: `robot.get_observation()` - è·å–æœºå™¨äººåŸå§‹è§‚å¯Ÿ

**æ„ä¹‰**ï¼š
- è·å–æœºå™¨äººçš„å½“å‰çŠ¶æ€ï¼ˆå…³èŠ‚ä½ç½®ã€é€Ÿåº¦ã€ç›¸æœºå›¾åƒç­‰ï¼‰

**ä¸ºä»€ä¹ˆéœ€è¦**ï¼š
1. **è®°å½•çŠ¶æ€**ï¼šç”¨äºè®­ç»ƒæ—¶çš„çŠ¶æ€-åŠ¨ä½œå¯¹
2. **å®æ—¶åé¦ˆ**ï¼šç”¨äºå¤„ç†å™¨å’Œé¥æ“ä½œå™¨
3. **å¤šæ¨¡æ€**ï¼šåŒ…å«å…³èŠ‚çŠ¶æ€å’Œè§†è§‰ä¿¡æ¯

**ç¤ºä¾‹**ï¼š
```python
obs = robot.get_observation()
# è¿”å›: {
#   "shoulder.pos": 0.5,
#   "elbow.pos": 0.3,
#   "camera_image": np.array([480, 640, 3])
# }
```

---

### æ­¥éª¤ 2: `robot_observation_processor(obs)` - å¤„ç†æœºå™¨äººè§‚å¯Ÿ

**æ„ä¹‰**ï¼š
- å°†æœºå™¨äººåŸå§‹è§‚å¯Ÿè½¬æ¢ä¸ºæ•°æ®é›†æ ‡å‡†æ ¼å¼

**ä¸ºä»€ä¹ˆéœ€è¦**ï¼š
1. **æ ¼å¼ç»Ÿä¸€**ï¼šä¸åŒæœºå™¨äººè¾“å‡ºæ ¼å¼ä¸åŒï¼Œéœ€è¦ç»Ÿä¸€
2. **åæ ‡è½¬æ¢**ï¼šä¾‹å¦‚å…³èŠ‚ä½ç½® â†’ æœ«ç«¯æ‰§è¡Œå™¨ä½å§¿ï¼ˆæ­£å‘è¿åŠ¨å­¦ï¼‰
3. **æ•°æ®å¢å¼º**ï¼šå½’ä¸€åŒ–ã€é‡å‘½åã€æ·»åŠ å‰ç¼€ç­‰
4. **ç‰¹å¾æå–**ï¼šæå–è®­ç»ƒæ‰€éœ€ç‰¹å¾

**å®é™…ä¾‹å­**ï¼ˆæ¥è‡ªæ–‡æ¡£ï¼‰ï¼š
```python
# Pipeline 3: Robot observation â†’ Dataset observation
robot_joints_to_ee_pose = RobotProcessorPipeline(
    steps=[
        ForwardKinematicsJointsToEE(kinematics=kinematics_solver)
        # å°†å…³èŠ‚ä½ç½®è½¬æ¢ä¸ºæœ«ç«¯æ‰§è¡Œå™¨ä½å§¿
    ]
)
```

**è½¬æ¢ç¤ºä¾‹**ï¼š
```python
# è¾“å…¥ï¼ˆæœºå™¨äººæ ¼å¼ï¼‰:
{
    "shoulder.pos": 0.5,
    "elbow.pos": 0.3
}

# è¾“å‡ºï¼ˆæ•°æ®é›†æ ¼å¼ï¼‰:
{
    "observation.state": [0.5, 0.3],  # å½’ä¸€åŒ–åçš„å…³èŠ‚ä½ç½®
    "observation.images.camera": image  # æ ‡å‡†åŒ–çš„å›¾åƒ
}
```

---

### æ­¥éª¤ 3: `teleop.get_action()` - ä»é¥æ“ä½œå™¨è·å–åŠ¨ä½œ

**æ„ä¹‰**ï¼š
- è¯»å–äººç±»æ“ä½œè€…çš„è¾“å…¥ï¼ˆä¸»åŠ¨è‡‚ä½ç½®ã€æ¸¸æˆæ‰‹æŸ„ã€æ‰‹æœºç­‰ï¼‰

**ä¸ºä»€ä¹ˆéœ€è¦**ï¼š
1. **æ¼”ç¤ºæ•°æ®**ï¼šè®°å½•äººç±»æ¼”ç¤ºç”¨äºæ¨¡ä»¿å­¦ä¹ 
2. **å®æ—¶æ§åˆ¶**ï¼šæ§åˆ¶æœºå™¨äººæ‰§è¡ŒåŠ¨ä½œ
3. **å¤šè®¾å¤‡æ”¯æŒ**ï¼šæ”¯æŒå¤šç§è¾“å…¥è®¾å¤‡

**ä¸åŒé¥æ“ä½œå™¨çš„è¾“å‡ºæ ¼å¼**ï¼š
```python
# SO-100 ä¸»åŠ¨è‡‚ï¼ˆå…³èŠ‚ä½ç½®ï¼‰
teleop.get_action() â†’ {"shoulder.pos": 0.5, "elbow.pos": 0.3}

# æ¸¸æˆæ‰‹æŸ„ï¼ˆå¢é‡æ§åˆ¶ï¼‰
teleop.get_action() â†’ {"delta_x": 0.1, "delta_y": 0.0, "delta_z": -0.05}

# æ‰‹æœºï¼ˆå§¿æ€ï¼‰
teleop.get_action() â†’ {"pose": [x, y, z, qx, qy, qz, qw]}
```

---

### æ­¥éª¤ 4: `teleop_action_processor((act, obs))` - å¤„ç†é¥æ“ä½œå™¨åŠ¨ä½œ

**æ„ä¹‰**ï¼š
- å°†é¥æ“ä½œå™¨åŠ¨ä½œè½¬æ¢ä¸ºæ•°æ®é›†åŠ¨ä½œæ ¼å¼

**ä¸ºä»€ä¹ˆéœ€è¦**ï¼š
1. **æ ¼å¼è½¬æ¢**ï¼šä¸åŒé¥æ“ä½œå™¨è¾“å‡ºä¸åŒï¼Œéœ€ç»Ÿä¸€ä¸ºæ•°æ®é›†æ ¼å¼
2. **åæ ‡è½¬æ¢**ï¼šä¾‹å¦‚æ‰‹æœºå§¿æ€ â†’ æœ«ç«¯æ‰§è¡Œå™¨ç›®æ ‡ä½ç½®
3. **å¢é‡è½¬ç»å¯¹**ï¼šä¾‹å¦‚å¢é‡æ§åˆ¶ â†’ ç»å¯¹ä½ç½®
4. **å½’ä¸€åŒ–**ï¼šç»Ÿä¸€æ•°å€¼èŒƒå›´ï¼Œä¾¿äºè®­ç»ƒ

**å®é™…ä¾‹å­**ï¼ˆæ¥è‡ªæ–‡æ¡£ï¼‰ï¼š
```python
# Pipeline 1: Teleop action â†’ Dataset action
phone_to_robot_ee_pose_processor = RobotProcessorPipeline(
    steps=[
        MapPhoneActionToRobotAction(),  # æ‰‹æœºå§¿æ€ â†’ æœºå™¨äººæœ«ç«¯æ‰§è¡Œå™¨
        EEReferenceAndDelta(),          # è½¬æ¢ä¸ºç›¸å¯¹å¢é‡
        EEBoundsAndSafety(),            # å®‰å…¨é™åˆ¶
        GripperVelocityToJoint(),       # å¤¹çˆªé€Ÿåº¦è½¬æ¢
    ]
)
```

**è½¬æ¢ç¤ºä¾‹**ï¼š
```python
# è¾“å…¥ï¼ˆé¥æ“ä½œå™¨æ ¼å¼ - æ¸¸æˆæ‰‹æŸ„ï¼‰:
{
    "delta_x": 0.1,
    "delta_y": 0.0,
    "delta_z": -0.05,
    "gripper": 1.0
}

# è¾“å‡ºï¼ˆæ•°æ®é›†æ ¼å¼ï¼‰:
{
    "action.ee.target_x": 0.5,  # è½¬æ¢ä¸ºç»å¯¹ä½ç½®
    "action.ee.target_y": 0.3,
    "action.ee.target_z": 0.2,
    "action.gripper": 1.0
}
```

---

### æ­¥éª¤ 5: `robot_action_processor((act_processed, obs))` - å¤„ç†æœºå™¨äººåŠ¨ä½œ

**æ„ä¹‰**ï¼š
- å°†æ•°æ®é›†åŠ¨ä½œæ ¼å¼è½¬æ¢ä¸ºæœºå™¨äººå¯æ‰§è¡Œçš„å‘½ä»¤

**ä¸ºä»€ä¹ˆéœ€è¦**ï¼š
1. **é€†è¿åŠ¨å­¦**ï¼šæœ«ç«¯æ‰§è¡Œå™¨ç›®æ ‡ â†’ å…³èŠ‚ç›®æ ‡ä½ç½®
2. **å®‰å…¨é™åˆ¶**ï¼šé€Ÿåº¦é™åˆ¶ã€ä½ç½®é™åˆ¶ã€ç¢°æ’æ£€æµ‹
3. **æ ¼å¼é€‚é…**ï¼šæ•°æ®é›†æ ¼å¼ â†’ æœºå™¨äººç”µæœºå‘½ä»¤æ ¼å¼
4. **å®æ—¶è°ƒæ•´**ï¼šæ ¹æ®å½“å‰çŠ¶æ€è°ƒæ•´åŠ¨ä½œ

**å®é™…ä¾‹å­**ï¼ˆæ¥è‡ªæ–‡æ¡£ï¼‰ï¼š
```python
# Pipeline 2: Dataset action â†’ Robot command
robot_ee_to_joints_processor = RobotProcessorPipeline(
    steps=[
        InverseKinematicsEEToJoints(kinematics=kinematics_solver)
        # æœ«ç«¯æ‰§è¡Œå™¨ç›®æ ‡ â†’ å…³èŠ‚ç›®æ ‡ä½ç½®
    ]
)
```

**è½¬æ¢ç¤ºä¾‹**ï¼š
```python
# è¾“å…¥ï¼ˆæ•°æ®é›†æ ¼å¼ï¼‰:
{
    "action.ee.target_x": 0.5,
    "action.ee.target_y": 0.3,
    "action.ee.target_z": 0.2
}

# è¾“å‡ºï¼ˆæœºå™¨äººå‘½ä»¤æ ¼å¼ï¼‰:
{
    "shoulder.pos": 0.45,  # é€šè¿‡é€†è¿åŠ¨å­¦è®¡ç®—
    "elbow.pos": 0.62,
    "wrist.pos": 0.31
}
```

**å®‰å…¨é™åˆ¶ç¤ºä¾‹**ï¼ˆæ¥è‡ªä»£ç ï¼‰ï¼š
```python
# åœ¨ robot.send_action() ä¸­
if self.config.max_relative_target is not None:
    # é™åˆ¶æœ€å¤§ç›¸å¯¹ç§»åŠ¨ï¼Œé˜²æ­¢çªç„¶å¤§å¹…ç§»åŠ¨
    goal_pos = ensure_safe_goal_position(goal_pos, present_pos, max_relative_target)
```

---

## å®Œæ•´æ•°æ®æµç¤ºä¾‹

### åœºæ™¯ï¼šä½¿ç”¨æ‰‹æœºæ§åˆ¶ SO-100 æœºå™¨äºº

```
[1] æœºå™¨äººè§‚å¯Ÿ
robot.get_observation()
â†’ {"shoulder.pos": 0.5, "elbow.pos": 0.3, "camera": image}

[2] å¤„ç†è§‚å¯Ÿ
robot_observation_processor(obs)
â†’ ForwardKinematicsJointsToEE()
â†’ {"observation.ee.x": 0.4, "observation.ee.y": 0.2, "observation.images.camera": image}

[3] é¥æ“ä½œå™¨åŠ¨ä½œ
phone.get_action()
â†’ {"pose": [x, y, z, qx, qy, qz, qw]}  # æ‰‹æœºå§¿æ€

[4] å¤„ç†é¥æ“ä½œå™¨åŠ¨ä½œ
teleop_action_processor((act, obs))
â†’ MapPhoneActionToRobotAction()  # æ‰‹æœºå§¿æ€ â†’ æœ«ç«¯æ‰§è¡Œå™¨ç›®æ ‡
â†’ EEReferenceAndDelta()           # è½¬æ¢ä¸ºç›¸å¯¹å¢é‡
â†’ {"action.ee.target_x": 0.1, "action.ee.target_y": 0.05}

[5] å¤„ç†æœºå™¨äººåŠ¨ä½œ
robot_action_processor((act_processed, obs))
â†’ InverseKinematicsEEToJoints()  # æœ«ç«¯æ‰§è¡Œå™¨ â†’ å…³èŠ‚ä½ç½®
â†’ {"shoulder.pos": 0.52, "elbow.pos": 0.35}

[6] å‘é€åˆ°æœºå™¨äºº
robot.send_action(robot_action)
â†’ ç”µæœºæ‰§è¡ŒåŠ¨ä½œ

[7] ä¿å­˜åˆ°æ•°æ®é›†
dataset.add_frame({
    "observation.ee.x": 0.4,
    "observation.ee.y": 0.2,
    "action.ee.target_x": 0.1,
    "action.ee.target_y": 0.05
})
```

---

## è®¾è®¡ä¼˜åŠ¿

### 1. æ¨¡å—åŒ–
- æ¯ä¸ªå¤„ç†å™¨èŒè´£å•ä¸€ï¼Œæ˜“äºç»´æŠ¤å’Œæ‰©å±•

### 2. å¯ç»„åˆæ€§
- å¯ä»¥ç»„åˆä¸åŒçš„å¤„ç†å™¨æ­¥éª¤ï¼Œé€‚åº”ä¸åŒåœºæ™¯

### 3. å¯å¤ç”¨æ€§
- åŒä¸€å¤„ç†å™¨å¯ç”¨äºä¸åŒæœºå™¨äºº/é¥æ“ä½œå™¨ç»„åˆ

### 4. å¯æµ‹è¯•æ€§
- æ¯ä¸ªå¤„ç†å™¨å¯ç‹¬ç«‹æµ‹è¯•

### 5. çµæ´»æ€§
- å¯ä»¥è½»æ¾æ·»åŠ æ–°çš„è½¬æ¢æ­¥éª¤ï¼ˆå¦‚æ»¤æ³¢ã€å¹³æ»‘ç­‰ï¼‰

---

## ä¸ºä»€ä¹ˆä¸èƒ½ç›´æ¥ä½¿ç”¨ï¼Ÿ

å¦‚æœè·³è¿‡å¤„ç†å™¨ï¼Œä¼šé‡åˆ°ï¼š

1. **æ ¼å¼ä¸åŒ¹é…**ï¼šé¥æ“ä½œå™¨è¾“å‡ºä¸æ•°æ®é›†æ ¼å¼ä¸ä¸€è‡´
2. **åæ ‡ç³»ç»Ÿä¸åŒ**ï¼šéœ€è¦åæ ‡è½¬æ¢ï¼ˆå…³èŠ‚ â†” æœ«ç«¯æ‰§è¡Œå™¨ï¼‰
3. **å®‰å…¨é£é™©**ï¼šæ²¡æœ‰å®‰å…¨é™åˆ¶å¯èƒ½å¯¼è‡´å±é™©åŠ¨ä½œ
4. **è®­ç»ƒå›°éš¾**ï¼šæœªå½’ä¸€åŒ–çš„æ•°æ®éš¾ä»¥è®­ç»ƒ
5. **å…¼å®¹æ€§å·®**ï¼šæ›´æ¢è®¾å¤‡éœ€è¦é‡å†™å¤§é‡ä»£ç 

---

## å…³é”®æ¦‚å¿µæ€»ç»“

### ä¸‰ä¸ªå¤„ç†å™¨ç®¡é“

1. **Teleop Action Processor**: é¥æ“ä½œå™¨åŠ¨ä½œ â†’ æ•°æ®é›†åŠ¨ä½œ
   - æ ¼å¼è½¬æ¢
   - åæ ‡è½¬æ¢
   - å½’ä¸€åŒ–

2. **Robot Action Processor**: æ•°æ®é›†åŠ¨ä½œ â†’ æœºå™¨äººå‘½ä»¤
   - é€†è¿åŠ¨å­¦
   - å®‰å…¨é™åˆ¶
   - æ ¼å¼é€‚é…

3. **Robot Observation Processor**: æœºå™¨äººè§‚å¯Ÿ â†’ æ•°æ®é›†è§‚å¯Ÿ
   - æ­£å‘è¿åŠ¨å­¦
   - æ ¼å¼ç»Ÿä¸€
   - ç‰¹å¾æå–

### æ•°æ®æ ¼å¼è½¬æ¢é“¾

```
åŸå§‹ç¡¬ä»¶æ•°æ® â†’ æ ‡å‡†åŒ–æ•°æ® â†’ è®­ç»ƒæ•°æ®
     â†“              â†“            â†“
  æœºå™¨äºº/é¥æ“ä½œå™¨ â†’ å¤„ç†å™¨ç®¡é“ â†’ æ•°æ®é›†
```

---

## å­¦ä¹ èµ„æº

### ç›¸å…³æ–‡æ¡£

- `LEARNING_STEPS.md` - å®Œæ•´å­¦ä¹ æ­¥éª¤æŒ‡å—
- `PRACTICE_GUIDE.md` - å®è·µæŒ‡å—
- `NEXT_STEPS.md` - ä¸‹ä¸€æ­¥è¡ŒåŠ¨
- `PROJECT_MODULES.md` - é¡¹ç›®æ¨¡å—è¯´æ˜
- `UNDERSTANDING_PIP_INSTALL.md` - pip install è¯´æ˜

### å®˜æ–¹èµ„æº

- ğŸ“š **å®˜æ–¹æ–‡æ¡£**: https://huggingface.co/docs/lerobot
- ğŸ’¬ **Discord ç¤¾åŒº**: https://discord.gg/s3KuuzsPFb
- ğŸ› **GitHub Issues**: https://github.com/huggingface/lerobot/issues
- ğŸ“¦ **æ•°æ®é›† Hub**: https://huggingface.co/lerobot

---

## é‡è¦å‘½ä»¤

### å®‰è£…

```bash
# å¯ç¼–è¾‘å®‰è£…ï¼ˆæ¨èå¼€å‘ï¼‰
pip install -e .

# ä» PyPI å®‰è£…
pip install lerobot
```

### æ•°æ®å½•åˆ¶

```bash
lerobot-record \
    --robot.type=so100_follower \
    --robot.port=/dev/tty.usbmodemXXX \
    --dataset.repo_id=your_username/your_dataset \
    --teleop.type=so100_leader
```

### è®­ç»ƒç­–ç•¥

```bash
lerobot-train \
    --dataset.repo_id=your_username/your_dataset \
    --policy.type=act \
    --output_dir=outputs/train/my_policy
```

---

**æ–‡æ¡£ç”Ÿæˆæ—¶é—´**: 2024å¹´11æœˆ
**LeRobot ç‰ˆæœ¬**: 0.4.2

