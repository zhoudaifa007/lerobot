# LeRobot æ¡†æ¶å¦‚ä½•å®šä¹‰å¥–åŠ±å‡½æ•°

æœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜ LeRobot æ¡†æ¶ä¸­å¥–åŠ±å‡½æ•°çš„å®šä¹‰æœºåˆ¶å’Œä½¿ç”¨æ–¹å¼ã€‚

## ğŸ“‹ ç›®å½•

1. [æ¡†æ¶çš„å¥–åŠ±å‡½æ•°æœºåˆ¶](#æ¡†æ¶çš„å¥–åŠ±å‡½æ•°æœºåˆ¶)
2. [å¥–åŠ±å‡½æ•°çš„æ¥æº](#å¥–åŠ±å‡½æ•°çš„æ¥æº)
3. [å¥–åŠ±å¤„ç†æµç¨‹](#å¥–åŠ±å¤„ç†æµç¨‹)
4. [å¦‚ä½•è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°](#å¦‚ä½•è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°)
5. [ä»£ç ç¤ºä¾‹](#ä»£ç ç¤ºä¾‹)

---

## æ¡†æ¶çš„å¥–åŠ±å‡½æ•°æœºåˆ¶

### æ ¸å¿ƒåŸåˆ™

LeRobot æ¡†æ¶é‡‡ç”¨**åˆ†å±‚å¥–åŠ±æœºåˆ¶**ï¼š

1. **ç¯å¢ƒå±‚**ï¼šåº•å±‚ç¯å¢ƒï¼ˆå¦‚ Gymã€Metaworldã€LIBEROï¼‰è®¡ç®—åŸºç¡€å¥–åŠ±
2. **å¤„ç†å™¨å±‚**ï¼šå¥–åŠ±å¤„ç†å™¨å¯ä»¥ä¿®æ”¹æˆ–å¢å¼ºå¥–åŠ±
3. **æœ€ç»ˆå¥–åŠ±**ï¼šç¯å¢ƒå¥–åŠ± + å¤„ç†å™¨å¥–åŠ±

### å¥–åŠ±è®¡ç®—æµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ç¯å¢ƒ (Env)    â”‚
â”‚  env.step()     â”‚
â”‚       â†“         â”‚
â”‚  åŸºç¡€å¥–åŠ±è®¡ç®—    â”‚
â”‚  reward_env     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  åŠ¨ä½œå¤„ç†å™¨      â”‚
â”‚  action_processorâ”‚
â”‚       â†“         â”‚
â”‚  å¤„ç†å™¨å¥–åŠ±      â”‚
â”‚  reward_proc    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  æœ€ç»ˆå¥–åŠ±        â”‚
â”‚  reward =        â”‚
â”‚  reward_env +    â”‚
â”‚  reward_proc     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## å¥–åŠ±å‡½æ•°çš„æ¥æº

### 1. ç¯å¢ƒå†…ç½®å¥–åŠ±å‡½æ•°

å¤§å¤šæ•°ç¯å¢ƒï¼ˆå¦‚ Metaworldã€LIBEROï¼‰åœ¨å†…éƒ¨å®šä¹‰äº†å¥–åŠ±å‡½æ•°ï¼š

```python
# src/lerobot/envs/metaworld.py
class MetaworldEnv(gym.Env):
    def step(self, action):
        # åº•å±‚ç¯å¢ƒè®¡ç®—å¥–åŠ±ï¼ˆå¥–åŠ±å‡½æ•°åœ¨ç¯å¢ƒå†…éƒ¨ï¼‰
        raw_obs, reward, done, truncated, info = self._env.step(action)
        #    â†‘ å¥–åŠ±ç”±åº•å±‚ Metaworld ç¯å¢ƒè®¡ç®—
        
        return observation, reward, terminated, truncated, info
```

```python
# src/lerobot/envs/libero.py
class LiberoEnv(gym.Env):
    def step(self, action):
        # åº•å±‚ç¯å¢ƒè®¡ç®—å¥–åŠ±ï¼ˆå¥–åŠ±å‡½æ•°åœ¨ç¯å¢ƒå†…éƒ¨ï¼‰
        raw_obs, reward, done, info = self._env.step(action)
        #    â†‘ å¥–åŠ±ç”±åº•å±‚ LIBERO ç¯å¢ƒè®¡ç®—
        
        return observation, reward, terminated, truncated, info
```

### 2. æœºå™¨äººç¯å¢ƒï¼ˆé›¶å¥–åŠ±ï¼‰

å¯¹äºçœŸå®æœºå™¨äººç¯å¢ƒï¼Œé»˜è®¤è¿”å›é›¶å¥–åŠ±ï¼š

```python
# src/lerobot/rl/gym_manipulator.py
class RobotEnv(gym.Env):
    def step(self, action):
        # æ‰§è¡ŒåŠ¨ä½œ
        self.robot.send_action(joint_targets_dict)
        obs = self._get_observation()
        
        # é»˜è®¤å¥–åŠ±ä¸º 0ï¼ˆéœ€è¦å¤–éƒ¨å®šä¹‰å¥–åŠ±å‡½æ•°ï¼‰
        reward = 0.0
        terminated = False
        truncated = False
        
        return obs, reward, terminated, truncated, info
```

### 3. å¤„ç†å™¨å¥–åŠ±

å¥–åŠ±å¯ä»¥é€šè¿‡å¤„ç†å™¨æ·»åŠ æˆ–ä¿®æ”¹ï¼š

```python
# src/lerobot/rl/gym_manipulator.py
def step_env_and_process_transition(...):
    # 1. ç¯å¢ƒè®¡ç®—å¥–åŠ±
    obs, reward, terminated, truncated, info = env.step(processed_action)
    #    â†‘ ç¯å¢ƒå¥–åŠ±
    
    # 2. å¤„ç†å™¨å¯èƒ½æ·»åŠ é¢å¤–å¥–åŠ±
    reward = reward + processed_action_transition[TransitionKey.REWARD]
    #    â†‘ ç¯å¢ƒå¥–åŠ± + å¤„ç†å™¨å¥–åŠ±
```

---

## å¥–åŠ±å¤„ç†æµç¨‹

### å®Œæ•´æµç¨‹

```python
# src/lerobot/rl/gym_manipulator.py
def step_env_and_process_transition(
    env: gym.Env,
    transition: EnvTransition,
    action: torch.Tensor,
    env_processor: DataProcessorPipeline,
    action_processor: DataProcessorPipeline,
) -> EnvTransition:
    # 1. åˆ›å»ºåŠ¨ä½œè½¬æ¢
    transition[TransitionKey.ACTION] = action
    transition[TransitionKey.OBSERVATION] = env.get_raw_joint_positions()
    
    # 2. é€šè¿‡åŠ¨ä½œå¤„ç†å™¨å¤„ç†ï¼ˆå¯èƒ½æ·»åŠ å¥–åŠ±ï¼‰
    processed_action_transition = action_processor(transition)
    processed_action = processed_action_transition[TransitionKey.ACTION]
    
    # 3. ç¯å¢ƒæ‰§è¡Œæ­¥éª¤ï¼ˆç¯å¢ƒè®¡ç®—å¥–åŠ±ï¼‰
    obs, reward, terminated, truncated, info = env.step(processed_action)
    #    â†‘ ç¯å¢ƒå¥–åŠ±å‡½æ•°è®¡ç®—çš„å¥–åŠ±
    
    # 4. åˆå¹¶å¥–åŠ±ï¼ˆç¯å¢ƒå¥–åŠ± + å¤„ç†å™¨å¥–åŠ±ï¼‰
    reward = reward + processed_action_transition[TransitionKey.REWARD]
    #    â†‘ æœ€ç»ˆå¥–åŠ± = ç¯å¢ƒå¥–åŠ± + å¤„ç†å™¨å¥–åŠ±
    
    # 5. åˆ›å»ºæ–°è½¬æ¢
    new_transition = create_transition(
        observation=obs,
        action=processed_action,
        reward=reward,  # æœ€ç»ˆå¥–åŠ±
        done=terminated,
        truncated=truncated,
        info=info,
    )
    
    # 6. é€šè¿‡ç¯å¢ƒå¤„ç†å™¨å¤„ç†ï¼ˆå¯èƒ½ä¿®æ”¹å¥–åŠ±ï¼‰
    new_transition = env_processor(new_transition)
    
    return new_transition
```

### å…³é”®ç‚¹

1. **ç¯å¢ƒå¥–åŠ±**ï¼šç”± `env.step()` è¿”å›
2. **å¤„ç†å™¨å¥–åŠ±**ï¼šç”± `action_processor` åœ¨ `TransitionKey.REWARD` ä¸­æ·»åŠ 
3. **æœ€ç»ˆå¥–åŠ±**ï¼š`ç¯å¢ƒå¥–åŠ± + å¤„ç†å™¨å¥–åŠ±`
4. **åå¤„ç†**ï¼š`env_processor` å¯ä»¥è¿›ä¸€æ­¥ä¿®æ”¹å¥–åŠ±

---

## å¦‚ä½•è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°

### æ–¹æ³• 1ï¼šåœ¨è‡ªå®šä¹‰ç¯å¢ƒä¸­å®šä¹‰

```python
import gymnasium as gym
import numpy as np

class CustomRobotEnv(gym.Env):
    """è‡ªå®šä¹‰æœºå™¨äººç¯å¢ƒï¼ŒåŒ…å«å¥–åŠ±å‡½æ•°"""
    
    def __init__(self):
        super().__init__()
        self.goal = np.array([0.5, 0.5, 0.5])
        self.state = None
        
    def reset(self):
        self.state = np.random.rand(3)
        return self.state
    
    def step(self, action):
        # æ‰§è¡ŒåŠ¨ä½œ
        self.state = self.state + action * 0.1
        
        # å®šä¹‰å¥–åŠ±å‡½æ•°
        reward = self._compute_reward(self.state, action)
        
        # æ£€æŸ¥æ˜¯å¦å®Œæˆ
        done = np.linalg.norm(self.state - self.goal) < 0.1
        
        return self.state, reward, done, False, {}
    
    def _compute_reward(self, state, action):
        """è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°"""
        # 1. è·ç¦»ç›®æ ‡çš„å¥–åŠ±
        distance = np.linalg.norm(state - self.goal)
        distance_reward = -distance
        
        # 2. ä»»åŠ¡å®Œæˆçš„å¥–åŠ±
        if distance < 0.1:
            completion_reward = 10.0
        else:
            completion_reward = 0.0
        
        # 3. åŠ¨ä½œå¹³æ»‘æ€§å¥–åŠ±
        action_penalty = -0.1 * np.linalg.norm(action)
        
        # æ€»å¥–åŠ±
        total_reward = distance_reward + completion_reward + action_penalty
        
        return total_reward
```

### æ–¹æ³• 2ï¼šä½¿ç”¨å¥–åŠ±å¤„ç†å™¨

```python
from lerobot.processor.pipeline import RewardProcessorStep
from lerobot.processor.core import TransitionKey, EnvTransition

class DistanceRewardProcessor(RewardProcessorStep):
    """åŸºäºè·ç¦»çš„å¥–åŠ±å¤„ç†å™¨"""
    
    def __init__(self, goal, distance_scale=1.0):
        self.goal = goal
        self.distance_scale = distance_scale
    
    def reward(self, reward):
        """ä¿®æ”¹å¥–åŠ±å€¼"""
        # ä»è½¬æ¢ä¸­è·å–çŠ¶æ€ï¼ˆéœ€è¦è®¿é—®å½“å‰è½¬æ¢ï¼‰
        transition = self._current_transition
        observation = transition.get(TransitionKey.OBSERVATION)
        
        if observation is None:
            return reward
        
        # è®¡ç®—è·ç¦»å¥–åŠ±
        state = observation.get("state", None)
        if state is not None:
            distance = np.linalg.norm(state - self.goal)
            distance_reward = -distance * self.distance_scale
            return reward + distance_reward
        
        return reward
```

### æ–¹æ³• 3ï¼šä½¿ç”¨å¥–åŠ±åˆ†ç±»å™¨

LeRobot æä¾›äº† `RewardClassifierProcessorStep`ï¼Œå¯ä»¥æ ¹æ®å›¾åƒåˆ†ç±»å™¨é¢„æµ‹æˆåŠŸï¼š

```python
# åœ¨é…ç½®ä¸­ä½¿ç”¨
@dataclass
class RewardClassifierProcessorStep(ProcessorStep):
    """ä½¿ç”¨å¥–åŠ±åˆ†ç±»å™¨ä¿®æ”¹å¥–åŠ±"""
    
    pretrained_path: str
    success_threshold: float = 0.5
    success_reward: float = 1.0
    terminate_on_success: bool = True
    
    def __call__(self, transition: EnvTransition) -> EnvTransition:
        # 1. ä»è§‚å¯Ÿä¸­æå–å›¾åƒ
        images = extract_images(transition)
        
        # 2. ä½¿ç”¨åˆ†ç±»å™¨é¢„æµ‹æˆåŠŸ
        success = self.reward_classifier.predict_reward(
            images, 
            threshold=self.success_threshold
        )
        
        # 3. æ ¹æ®é¢„æµ‹ä¿®æ”¹å¥–åŠ±
        if success >= self.success_threshold:
            reward = self.success_reward
            if self.terminate_on_success:
                terminated = True
        
        # 4. æ›´æ–°è½¬æ¢
        transition[TransitionKey.REWARD] = reward
        return transition
```

### æ–¹æ³• 4ï¼šåœ¨åŠ¨ä½œå¤„ç†å™¨ä¸­æ·»åŠ å¥–åŠ±

```python
from lerobot.processor.pipeline import ActionProcessorStep
from lerobot.processor.core import TransitionKey

class PenaltyActionProcessor(ActionProcessorStep):
    """åŠ¨ä½œæƒ©ç½šå¤„ç†å™¨"""
    
    def __init__(self, penalty_scale=0.01):
        self.penalty_scale = penalty_scale
    
    def __call__(self, transition: EnvTransition) -> EnvTransition:
        new_transition = transition.copy()
        action = new_transition[TransitionKey.ACTION]
        
        # è®¡ç®—åŠ¨ä½œæƒ©ç½šï¼ˆå¤§åŠ¨ä½œæƒ©ç½šï¼‰
        action_magnitude = torch.norm(action)
        penalty = -self.penalty_scale * action_magnitude
        
        # æ·»åŠ åˆ°è½¬æ¢çš„å¥–åŠ±ä¸­
        current_reward = new_transition.get(TransitionKey.REWARD, 0.0)
        new_transition[TransitionKey.REWARD] = current_reward + penalty
        
        return new_transition
```

---

## ä»£ç ç¤ºä¾‹

### ç¤ºä¾‹ 1ï¼šä½¿ç”¨ç¯å¢ƒå†…ç½®å¥–åŠ±å‡½æ•°

```python
from lerobot.envs.metaworld import MetaworldEnv

# åˆ›å»ºç¯å¢ƒï¼ˆå¥–åŠ±å‡½æ•°åœ¨ç¯å¢ƒå†…éƒ¨ï¼‰
env = MetaworldEnv(task="metaworld-reach-v2")

# æ‰§è¡Œæ­¥éª¤ï¼ˆç¯å¢ƒè‡ªåŠ¨è®¡ç®—å¥–åŠ±ï¼‰
obs, reward, done, truncated, info = env.step(action)
#    â†‘ å¥–åŠ±ç”± Metaworld ç¯å¢ƒçš„å¥–åŠ±å‡½æ•°è®¡ç®—
```

### ç¤ºä¾‹ 2ï¼šè‡ªå®šä¹‰ç¯å¢ƒå¥–åŠ±å‡½æ•°

```python
import gymnasium as gym
import numpy as np

class MyCustomEnv(gym.Env):
    def step(self, action):
        # æ‰§è¡ŒåŠ¨ä½œ
        next_state = self._execute_action(action)
        
        # è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°
        reward = self._compute_reward(self.state, action, next_state)
        
        return next_state, reward, done, info
    
    def _compute_reward(self, state, action, next_state):
        """è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°"""
        # å¥–åŠ±é€»è¾‘
        distance = np.linalg.norm(next_state - self.goal)
        reward = -distance
        
        if distance < 0.1:
            reward += 10.0
        
        return reward
```

### ç¤ºä¾‹ 3ï¼šä½¿ç”¨å¥–åŠ±å¤„ç†å™¨ä¿®æ”¹å¥–åŠ±

```python
from lerobot.processor.pipeline import RewardProcessorStep
from lerobot.processor.core import TransitionKey

class ScaleRewardProcessor(RewardProcessorStep):
    """ç¼©æ”¾å¥–åŠ±å¤„ç†å™¨"""
    
    def __init__(self, scale: float = 0.1):
        self.scale = scale
    
    def reward(self, reward):
        """ç¼©æ”¾å¥–åŠ±"""
        return reward * self.scale

# åœ¨å¤„ç†å™¨ç®¡é“ä¸­ä½¿ç”¨
processor = ScaleRewardProcessor(scale=0.1)
processed_reward = processor.reward(original_reward)
```

### ç¤ºä¾‹ 4ï¼šåœ¨åŠ¨ä½œå¤„ç†å™¨ä¸­æ·»åŠ å¥–åŠ±

```python
from lerobot.processor.pipeline import ActionProcessorStep
from lerobot.processor.core import TransitionKey
import torch

class SmoothActionProcessor(ActionProcessorStep):
    """å¹³æ»‘åŠ¨ä½œå¥–åŠ±å¤„ç†å™¨"""
    
    def __call__(self, transition: EnvTransition) -> EnvTransition:
        new_transition = transition.copy()
        action = new_transition[TransitionKey.ACTION]
        
        # è®¡ç®—å¹³æ»‘æ€§å¥–åŠ±ï¼ˆæƒ©ç½šå¤§åŠ¨ä½œï¼‰
        action_magnitude = torch.norm(action)
        smoothness_reward = -0.01 * action_magnitude
        
        # æ·»åŠ åˆ°è½¬æ¢çš„å¥–åŠ±ä¸­
        current_reward = new_transition.get(TransitionKey.REWARD, 0.0)
        new_transition[TransitionKey.REWARD] = current_reward + smoothness_reward
        
        return new_transition
```

### ç¤ºä¾‹ 5ï¼šå®Œæ•´çš„å¥–åŠ±å¤„ç†æµç¨‹

```python
from lerobot.rl.gym_manipulator import step_env_and_process_transition
from lerobot.processor.pipeline import DataProcessorPipeline

# åˆ›å»ºç¯å¢ƒ
env = MyCustomEnv()

# åˆ›å»ºå¤„ç†å™¨
action_processor = DataProcessorPipeline([
    SmoothActionProcessor(),  # æ·»åŠ å¹³æ»‘æ€§å¥–åŠ±
])

env_processor = DataProcessorPipeline([
    ScaleRewardProcessor(scale=0.1),  # ç¼©æ”¾å¥–åŠ±
])

# æ‰§è¡Œæ­¥éª¤
transition = create_transition(...)
new_transition = step_env_and_process_transition(
    env=env,
    transition=transition,
    action=action,
    env_processor=env_processor,
    action_processor=action_processor,
)

# æœ€ç»ˆå¥–åŠ± = ç¯å¢ƒå¥–åŠ± + åŠ¨ä½œå¤„ç†å™¨å¥–åŠ±ï¼Œç„¶åé€šè¿‡ç¯å¢ƒå¤„ç†å™¨ç¼©æ”¾
final_reward = new_transition[TransitionKey.REWARD]
```

---

## æ¡†æ¶çš„å¥–åŠ±å‡½æ•°æ€»ç»“

### å¥–åŠ±å‡½æ•°çš„å®šä¹‰ä½ç½®

1. **ç¯å¢ƒå†…éƒ¨**ï¼ˆæœ€å¸¸è§ï¼‰
   - Metaworldã€LIBERO ç­‰ç¯å¢ƒåœ¨å†…éƒ¨å®šä¹‰å¥–åŠ±å‡½æ•°
   - é€šè¿‡ `env.step()` è¿”å›å¥–åŠ±å€¼

2. **è‡ªå®šä¹‰ç¯å¢ƒ**
   - åœ¨ `step()` æ–¹æ³•ä¸­å®ç° `_compute_reward()` æ–¹æ³•
   - æ ¹æ®çŠ¶æ€ã€åŠ¨ä½œã€ä¸‹ä¸€ä¸ªçŠ¶æ€è®¡ç®—å¥–åŠ±

3. **å¥–åŠ±å¤„ç†å™¨**
   - é€šè¿‡ `RewardProcessorStep` ä¿®æ”¹å¥–åŠ±
   - é€šè¿‡ `ActionProcessorStep` æ·»åŠ å¥–åŠ±

### å¥–åŠ±è®¡ç®—æµç¨‹

```
ç¯å¢ƒ step() â†’ ç¯å¢ƒå¥–åŠ±
     â†“
åŠ¨ä½œå¤„ç†å™¨ â†’ å¤„ç†å™¨å¥–åŠ±
     â†“
æœ€ç»ˆå¥–åŠ± = ç¯å¢ƒå¥–åŠ± + å¤„ç†å™¨å¥–åŠ±
     â†“
ç¯å¢ƒå¤„ç†å™¨ â†’ å¯èƒ½è¿›ä¸€æ­¥ä¿®æ”¹å¥–åŠ±
```

### å…³é”®ä»£ç ä½ç½®

1. **ç¯å¢ƒå¥–åŠ±**ï¼š
   - `src/lerobot/envs/metaworld.py` - MetaworldEnv.step()
   - `src/lerobot/envs/libero.py` - LiberoEnv.step()
   - `src/lerobot/rl/gym_manipulator.py` - RobotEnv.step()

2. **å¥–åŠ±å¤„ç†**ï¼š
   - `src/lerobot/rl/gym_manipulator.py` - step_env_and_process_transition()
   - `src/lerobot/processor/pipeline.py` - RewardProcessorStep
   - `src/lerobot/processor/hil_processor.py` - RewardClassifierProcessorStep

### æœ€ä½³å®è·µ

1. **ä½¿ç”¨ç¯å¢ƒå†…ç½®å¥–åŠ±**ï¼šå¯¹äºæ ‡å‡†ç¯å¢ƒï¼ˆMetaworldã€LIBEROï¼‰ï¼Œä½¿ç”¨ç¯å¢ƒå†…ç½®çš„å¥–åŠ±å‡½æ•°
2. **è‡ªå®šä¹‰ç¯å¢ƒå¥–åŠ±**ï¼šå¯¹äºè‡ªå®šä¹‰ä»»åŠ¡ï¼Œåœ¨ç¯å¢ƒçš„ `step()` æ–¹æ³•ä¸­å®šä¹‰å¥–åŠ±å‡½æ•°
3. **ä½¿ç”¨å¤„ç†å™¨å¢å¼º**ï¼šé€šè¿‡å¥–åŠ±å¤„ç†å™¨æ·»åŠ é¢å¤–çš„å¥–åŠ±ä¿¡å·ï¼ˆå¦‚å¹³æ»‘æ€§ã€å®‰å…¨æ€§ï¼‰
4. **å¥–åŠ±åˆ†ç±»å™¨**ï¼šå¯¹äºéš¾ä»¥å®šä¹‰å¥–åŠ±çš„ä»»åŠ¡ï¼Œä½¿ç”¨å¥–åŠ±åˆ†ç±»å™¨ä»å›¾åƒé¢„æµ‹æˆåŠŸ

---

è¿™äº›æœºåˆ¶ä½¿å¾— LeRobot æ¡†æ¶èƒ½å¤Ÿçµæ´»åœ°å¤„ç†å„ç§å¥–åŠ±å‡½æ•°å®šä¹‰æ–¹å¼ï¼

