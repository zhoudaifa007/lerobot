# å¥–åŠ±å‡½æ•° vs å¥–åŠ±ç‰¹å¾

æœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜å¥–åŠ±å‡½æ•°å’Œå¥–åŠ±ç‰¹å¾çš„åŒºåˆ«ï¼Œä»¥åŠå¦‚ä½•åœ¨ LeRobot ä¸­å®šä¹‰å’Œä½¿ç”¨å®ƒä»¬ã€‚

## ğŸ“‹ ç›®å½•

1. [æ ¸å¿ƒæ¦‚å¿µ](#æ ¸å¿ƒæ¦‚å¿µ)
2. [å¥–åŠ±å‡½æ•°ï¼ˆReward Functionï¼‰](#å¥–åŠ±å‡½æ•°reward-function)
3. [å¥–åŠ±ç‰¹å¾ï¼ˆReward Featureï¼‰](#å¥–åŠ±ç‰¹å¾reward-feature)
4. [ä¸¤è€…çš„å…³ç³»](#ä¸¤è€…çš„å…³ç³»)
5. [å¦‚ä½•å®šä¹‰å¥–åŠ±å‡½æ•°](#å¦‚ä½•å®šä¹‰å¥–åŠ±å‡½æ•°)
6. [å¥–åŠ±å¤„ç†å™¨](#å¥–åŠ±å¤„ç†å™¨)
7. [ä»£ç ç¤ºä¾‹](#ä»£ç ç¤ºä¾‹)

---

## æ ¸å¿ƒæ¦‚å¿µ

### å¥–åŠ±å‡½æ•° vs å¥–åŠ±ç‰¹å¾

| ç‰¹æ€§ | å¥–åŠ±å‡½æ•° | å¥–åŠ±ç‰¹å¾ |
|------|---------|---------|
| **å®šä¹‰** | è®¡ç®—å¥–åŠ±çš„æ•°å­¦å‡½æ•° | æ•°æ®é›†ä¸­å­˜å‚¨çš„å¥–åŠ±å€¼ |
| **ä½ç½®** | ç¯å¢ƒï¼ˆEnvironmentï¼‰ä¸­ | æ•°æ®é›†ï¼ˆDatasetï¼‰ä¸­ |
| **æ—¶æœº** | è¿è¡Œæ—¶è®¡ç®— | å·²è®¡ç®—å¹¶å­˜å‚¨ |
| **è¾“å…¥** | `(state, action, next_state)` | æ— ï¼ˆå·²ç»æ˜¯å€¼ï¼‰ |
| **è¾“å‡º** | æ ‡é‡å¥–åŠ±å€¼ | æ ‡é‡å¥–åŠ±å€¼ |
| **ç”¨é€”** | å¼ºåŒ–å­¦ä¹ è®­ç»ƒ/è¯„ä¼° | ç¦»çº¿è®­ç»ƒã€æ•°æ®åˆ†æ |

---

## å¥–åŠ±å‡½æ•°ï¼ˆReward Functionï¼‰

### å®šä¹‰

**å¥–åŠ±å‡½æ•°**æ˜¯åœ¨ç¯å¢ƒä¸­å®šä¹‰çš„å‡½æ•°ï¼Œç”¨äºæ ¹æ®å½“å‰çŠ¶æ€ã€åŠ¨ä½œå’Œä¸‹ä¸€ä¸ªçŠ¶æ€è®¡ç®—å¥–åŠ±å€¼ã€‚

### ç‰¹ç‚¹

1. **è¿è¡Œæ—¶è®¡ç®—**ï¼šæ¯æ¬¡ç¯å¢ƒæ‰§è¡Œ `step()` æ—¶è®¡ç®—
2. **åŠ¨æ€æ€§**ï¼šå¯ä»¥æ ¹æ®ç¯å¢ƒçŠ¶æ€å®æ—¶è®¡ç®—
3. **å¯ä¿®æ”¹**ï¼šå¯ä»¥é€šè¿‡å¥–åŠ±å¤„ç†å™¨ä¿®æ”¹
4. **ç¯å¢ƒç›¸å…³**ï¼šä¸åŒç¯å¢ƒæœ‰ä¸åŒçš„å¥–åŠ±å‡½æ•°

### åœ¨ç¯å¢ƒä¸­çš„å®ç°

å¥–åŠ±å‡½æ•°é€šå¸¸åœ¨ç¯å¢ƒçš„ `step()` æ–¹æ³•ä¸­è®¡ç®—ï¼š

```python
class MyEnv(gym.Env):
    def step(self, action):
        # 1. æ‰§è¡ŒåŠ¨ä½œ
        next_state = self._execute_action(action)
        
        # 2. è®¡ç®—å¥–åŠ±ï¼ˆå¥–åŠ±å‡½æ•°ï¼‰
        reward = self._compute_reward(self.state, action, next_state)
        
        # 3. æ£€æŸ¥æ˜¯å¦ç»“æŸ
        done = self._is_done(next_state)
        
        return next_state, reward, done, info
    
    def _compute_reward(self, state, action, next_state):
        """å¥–åŠ±å‡½æ•°ï¼šæ ¹æ®çŠ¶æ€å’ŒåŠ¨ä½œè®¡ç®—å¥–åŠ±"""
        # ç¤ºä¾‹ï¼šè·ç¦»ç›®æ ‡çš„å¥–åŠ±
        distance_to_goal = np.linalg.norm(next_state - self.goal)
        reward = -distance_to_goal  # è·ç¦»è¶Šè¿‘ï¼Œå¥–åŠ±è¶Šé«˜
        
        # ç¤ºä¾‹ï¼šä»»åŠ¡å®Œæˆçš„å¥–åŠ±
        if self._is_goal_reached(next_state):
            reward += 10.0
        
        # ç¤ºä¾‹ï¼šç¢°æ’æƒ©ç½š
        if self._is_collision(next_state):
            reward -= 5.0
        
        return reward
```

### LeRobot ä¸­çš„å¥–åŠ±å‡½æ•°ç¤ºä¾‹

#### 1. Metaworld ç¯å¢ƒ

```python
# src/lerobot/envs/metaworld.py
class MetaworldEnv(gym.Env):
    def step(self, action):
        # ç¯å¢ƒå†…éƒ¨è®¡ç®—å¥–åŠ±
        raw_obs, reward, done, truncated, info = self._env.step(action)
        
        # reward æ˜¯ç¯å¢ƒè¿”å›çš„å¥–åŠ±å€¼ï¼ˆç”±ç¯å¢ƒçš„å¥–åŠ±å‡½æ•°è®¡ç®—ï¼‰
        return observation, reward, terminated, truncated, info
```

#### 2. LIBERO ç¯å¢ƒ

```python
# src/lerobot/envs/libero.py
class LiberoEnv(gym.Env):
    def step(self, action):
        # ç¯å¢ƒå†…éƒ¨è®¡ç®—å¥–åŠ±
        raw_obs, reward, done, info = self._env.step(action)
        
        # reward æ˜¯ç¯å¢ƒè¿”å›çš„å¥–åŠ±å€¼
        return observation, reward, terminated, truncated, info
```

---

## å¥–åŠ±ç‰¹å¾ï¼ˆReward Featureï¼‰

### å®šä¹‰

**å¥–åŠ±ç‰¹å¾**æ˜¯æ•°æ®é›†ä¸­å­˜å‚¨çš„å¥–åŠ±å€¼ï¼Œæ˜¯å·²ç»è®¡ç®—å¥½çš„æ ‡é‡å€¼ã€‚

### ç‰¹ç‚¹

1. **é™æ€å­˜å‚¨**ï¼šå·²ç»è®¡ç®—å¹¶å­˜å‚¨åœ¨æ•°æ®é›†ä¸­
2. **ä¸å¯ä¿®æ”¹**ï¼šæ•°æ®é›†ä¸­çš„å€¼ä¸ä¼šæ”¹å˜
3. **ç¦»çº¿ä½¿ç”¨**ï¼šç”¨äºç¦»çº¿è®­ç»ƒå’Œæ•°æ®åˆ†æ
4. **æ ¼å¼å›ºå®š**ï¼šé€šå¸¸æ˜¯ `float32`ï¼Œå½¢çŠ¶ä¸º `(1,)`

### åœ¨æ•°æ®é›†ä¸­çš„å­˜å‚¨

```python
# æ•°æ®é›†ç‰¹å¾å®šä¹‰
features = {
    "reward": {
        "dtype": "float32",
        "shape": (1,),
        "names": None
    }
}

# æ•°æ®é›†ä¸­çš„å®é™…å€¼
sample = dataset[0]
reward = sample["reward"]  # ä¾‹å¦‚: tensor([0.5])
```

### ä»å¥–åŠ±å‡½æ•°åˆ°å¥–åŠ±ç‰¹å¾

åœ¨è®°å½•æ•°æ®æ—¶ï¼Œå¥–åŠ±å‡½æ•°è®¡ç®—çš„å¥–åŠ±å€¼ä¼šè¢«å­˜å‚¨ä¸ºå¥–åŠ±ç‰¹å¾ï¼š

```python
# è®°å½•å¾ªç¯
while recording:
    # 1. æ‰§è¡ŒåŠ¨ä½œ
    obs, reward, done, info = env.step(action)
    #    â†‘ å¥–åŠ±å‡½æ•°è®¡ç®—çš„å¥–åŠ±å€¼
    
    # 2. å­˜å‚¨åˆ°æ•°æ®é›†
    frame = {
        "observation.state": obs,
        "action": action,
        "reward": reward,  # â† å¥–åŠ±å‡½æ•°çš„å€¼å­˜å‚¨ä¸ºå¥–åŠ±ç‰¹å¾
        "done": done
    }
    dataset.add_frame(frame)
```

---

## ä¸¤è€…çš„å…³ç³»

### æµç¨‹å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ç¯å¢ƒ (Env)    â”‚
â”‚                 â”‚
â”‚  step(action)   â”‚
â”‚       â†“         â”‚
â”‚  å¥–åŠ±å‡½æ•°è®¡ç®—    â”‚
â”‚  reward = f(...) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“ reward å€¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  è®°å½•åˆ°æ•°æ®é›†    â”‚
â”‚                 â”‚
â”‚  dataset.add_   â”‚
â”‚  frame({        â”‚
â”‚    "reward":    â”‚
â”‚    reward       â”‚
â”‚  })             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“ å­˜å‚¨
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   æ•°æ®é›†å­˜å‚¨     â”‚
â”‚                 â”‚
â”‚  reward ç‰¹å¾     â”‚
â”‚  (é™æ€å€¼)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### å…³é”®åŒºåˆ«

1. **å¥–åŠ±å‡½æ•°**ï¼š
   - åœ¨ç¯å¢ƒä¸­å®šä¹‰
   - è¿è¡Œæ—¶è®¡ç®—
   - å¯ä»¥ä¿®æ”¹ï¼ˆé€šè¿‡å¤„ç†å™¨ï¼‰
   - ç”¨äºåœ¨çº¿å¼ºåŒ–å­¦ä¹ 

2. **å¥–åŠ±ç‰¹å¾**ï¼š
   - åœ¨æ•°æ®é›†ä¸­å­˜å‚¨
   - å·²ç»è®¡ç®—å¥½çš„å€¼
   - ä¸å¯ä¿®æ”¹
   - ç”¨äºç¦»çº¿è®­ç»ƒ

---

## å¦‚ä½•å®šä¹‰å¥–åŠ±å‡½æ•°

### æ–¹æ³• 1ï¼šåœ¨è‡ªå®šä¹‰ç¯å¢ƒä¸­å®šä¹‰

```python
import gym
import numpy as np

class CustomRobotEnv(gym.Env):
    def __init__(self):
        super().__init__()
        self.goal = np.array([0.5, 0.5, 0.5])
        self.state = None
        
    def reset(self):
        self.state = np.random.rand(3)
        return self.state
    
    def step(self, action):
        # æ‰§è¡ŒåŠ¨ä½œ
        self.state = self.state + action * 0.1
        
        # å®šä¹‰å¥–åŠ±å‡½æ•°
        reward = self._compute_reward(self.state, action)
        
        # æ£€æŸ¥æ˜¯å¦å®Œæˆ
        done = np.linalg.norm(self.state - self.goal) < 0.1
        
        return self.state, reward, done, {}
    
    def _compute_reward(self, state, action):
        """è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°"""
        # 1. è·ç¦»ç›®æ ‡çš„å¥–åŠ±
        distance = np.linalg.norm(state - self.goal)
        distance_reward = -distance
        
        # 2. ä»»åŠ¡å®Œæˆçš„å¥–åŠ±
        if distance < 0.1:
            completion_reward = 10.0
        else:
            completion_reward = 0.0
        
        # 3. åŠ¨ä½œå¹³æ»‘æ€§å¥–åŠ±ï¼ˆæƒ©ç½šå¤§åŠ¨ä½œï¼‰
        action_penalty = -0.1 * np.linalg.norm(action)
        
        # æ€»å¥–åŠ±
        total_reward = distance_reward + completion_reward + action_penalty
        
        return total_reward
```

### æ–¹æ³• 2ï¼šä½¿ç”¨å¥–åŠ±å¤„ç†å™¨ä¿®æ”¹å¥–åŠ±

```python
from lerobot.processor.pipeline import RewardProcessorStep

class CustomRewardProcessor(RewardProcessorStep):
    """è‡ªå®šä¹‰å¥–åŠ±å¤„ç†å™¨"""
    
    def reward(self, reward):
        """ä¿®æ”¹å¥–åŠ±å€¼"""
        # ä¾‹å¦‚ï¼šå°†å¥–åŠ±ç¼©æ”¾åˆ° [0, 1]
        normalized_reward = (reward + 1.0) / 2.0
        return normalized_reward
```

### æ–¹æ³• 3ï¼šä½¿ç”¨å¥–åŠ±åˆ†ç±»å™¨

LeRobot æä¾›äº† `RewardClassifierProcessorStep`ï¼Œå¯ä»¥æ ¹æ®å›¾åƒåˆ†ç±»å™¨é¢„æµ‹æˆåŠŸæ¥ä¿®æ”¹å¥–åŠ±ï¼š

```python
# src/lerobot/processor/hil_processor.py
@dataclass
class RewardClassifierProcessorStep(ProcessorStep):
    """ä½¿ç”¨å¥–åŠ±åˆ†ç±»å™¨ä¿®æ”¹å¥–åŠ±"""
    
    reward_classifier_path: str
    success_threshold: float = 0.5
    success_reward: float = 1.0
    terminate_on_success: bool = True
    
    def __call__(self, transition: EnvTransition) -> EnvTransition:
        # 1. ä»è§‚å¯Ÿä¸­æå–å›¾åƒ
        images = extract_images(transition)
        
        # 2. ä½¿ç”¨åˆ†ç±»å™¨é¢„æµ‹æˆåŠŸ
        success = self.reward_classifier.predict_reward(
            images, 
            threshold=self.success_threshold
        )
        
        # 3. æ ¹æ®é¢„æµ‹ä¿®æ”¹å¥–åŠ±
        if success >= self.success_threshold:
            reward = self.success_reward
            if self.terminate_on_success:
                terminated = True
        
        # 4. æ›´æ–°è½¬æ¢
        transition[TransitionKey.REWARD] = reward
        return transition
```

---

## å¥–åŠ±å¤„ç†å™¨

### ä»€ä¹ˆæ˜¯å¥–åŠ±å¤„ç†å™¨

å¥–åŠ±å¤„ç†å™¨æ˜¯å¯ä»¥åœ¨å¥–åŠ±å‡½æ•°è®¡ç®—åä¿®æ”¹å¥–åŠ±å€¼çš„ç»„ä»¶ã€‚

### å¤„ç†å™¨ç®¡é“

```
ç¯å¢ƒå¥–åŠ±å‡½æ•° â†’ å¥–åŠ±å¤„ç†å™¨ â†’ æœ€ç»ˆå¥–åŠ±å€¼
     â†“              â†“
  reward=0.5    reward=1.0
```

### ä½¿ç”¨ç¤ºä¾‹

```python
from lerobot.processor.pipeline import RewardProcessorStep

class ScaleRewardProcessor(RewardProcessorStep):
    """ç¼©æ”¾å¥–åŠ±å¤„ç†å™¨"""
    
    def __init__(self, scale: float = 0.1):
        self.scale = scale
    
    def reward(self, reward):
        """å°†å¥–åŠ±ç¼©æ”¾"""
        return reward * self.scale

# åœ¨é…ç½®ä¸­ä½¿ç”¨
processor = ScaleRewardProcessor(scale=0.1)
processed_reward = processor.reward(original_reward)
```

---

## ä»£ç ç¤ºä¾‹

### ç¤ºä¾‹ 1ï¼šå®šä¹‰ç®€å•çš„å¥–åŠ±å‡½æ•°

```python
import gym
import numpy as np

class SimpleReachEnv(gym.Env):
    """ç®€å•çš„åˆ°è¾¾ä»»åŠ¡ç¯å¢ƒ"""
    
    def __init__(self):
        super().__init__()
        self.goal = np.array([1.0, 1.0])
        self.state = None
        self.action_space = gym.spaces.Box(
            low=-1.0, high=1.0, shape=(2,), dtype=np.float32
        )
        self.observation_space = gym.spaces.Box(
            low=-np.inf, high=np.inf, shape=(2,), dtype=np.float32
        )
    
    def reset(self):
        self.state = np.array([0.0, 0.0])
        return self.state
    
    def step(self, action):
        # æ‰§è¡ŒåŠ¨ä½œ
        self.state = self.state + action * 0.1
        self.state = np.clip(self.state, -2.0, 2.0)
        
        # è®¡ç®—å¥–åŠ±ï¼ˆå¥–åŠ±å‡½æ•°ï¼‰
        reward = self._compute_reward(self.state)
        
        # æ£€æŸ¥æ˜¯å¦å®Œæˆ
        distance = np.linalg.norm(self.state - self.goal)
        done = distance < 0.1
        
        info = {"distance": distance}
        return self.state, reward, done, False, info
    
    def _compute_reward(self, state):
        """å¥–åŠ±å‡½æ•°å®šä¹‰"""
        # 1. è·ç¦»ç›®æ ‡çš„è´Ÿè·ç¦»ï¼ˆè·ç¦»è¶Šè¿‘ï¼Œå¥–åŠ±è¶Šé«˜ï¼‰
        distance = np.linalg.norm(state - self.goal)
        distance_reward = -distance
        
        # 2. åˆ°è¾¾ç›®æ ‡çš„å¥–åŠ±
        if distance < 0.1:
            goal_reward = 10.0
        else:
            goal_reward = 0.0
        
        # 3. æ€»å¥–åŠ±
        total_reward = distance_reward + goal_reward
        
        return total_reward
```

### ç¤ºä¾‹ 2ï¼šè®°å½•æ•°æ®æ—¶å­˜å‚¨å¥–åŠ±ç‰¹å¾

```python
from lerobot.datasets.lerobot_dataset import LeRobotDataset
from lerobot.datasets.utils import build_dataset_frame
from lerobot.utils.constants import ACTION, OBS_STR, REWARD

# åˆ›å»ºç¯å¢ƒ
env = SimpleReachEnv()

# åˆ›å»ºæ•°æ®é›†
dataset = LeRobotDataset.create(
    repo_id="my_reach_dataset",
    fps=10,
    features={
        "observation.state": {"dtype": "float32", "shape": (2,)},
        "action": {"dtype": "float32", "shape": (2,)},
        "reward": {"dtype": "float32", "shape": (1,)},
        "done": {"dtype": "bool", "shape": (1,)},
    }
)

# è®°å½•æ•°æ®
obs = env.reset()
for step in range(100):
    # 1. é€‰æ‹©åŠ¨ä½œï¼ˆéšæœºæˆ–ç­–ç•¥ï¼‰
    action = env.action_space.sample()
    
    # 2. æ‰§è¡ŒåŠ¨ä½œï¼Œç¯å¢ƒè®¡ç®—å¥–åŠ±ï¼ˆå¥–åŠ±å‡½æ•°ï¼‰
    next_obs, reward, done, truncated, info = env.step(action)
    #    â†‘ å¥–åŠ±å‡½æ•°è®¡ç®—çš„å¥–åŠ±å€¼
    
    # 3. æ„å»ºæ•°æ®å¸§
    observation_frame = build_dataset_frame(
        dataset.features,
        {"state": obs},
        prefix=OBS_STR
    )
    action_frame = build_dataset_frame(
        dataset.features,
        action,
        prefix=ACTION
    )
    
    frame = {
        **observation_frame,
        **action_frame,
        "reward": np.array([reward], dtype=np.float32),  # å­˜å‚¨ä¸ºå¥–åŠ±ç‰¹å¾
        "done": np.array([done], dtype=bool),
        "task": "reach_goal"
    }
    
    # 4. æ·»åŠ åˆ°æ•°æ®é›†
    dataset.add_frame(frame)
    
    if done:
        dataset.save_episode()
        obs = env.reset()
    else:
        obs = next_obs
```

### ç¤ºä¾‹ 3ï¼šä»æ•°æ®é›†è¯»å–å¥–åŠ±ç‰¹å¾

```python
from lerobot.datasets.lerobot_dataset import LeRobotDataset

# åŠ è½½æ•°æ®é›†
dataset = LeRobotDataset("my_reach_dataset")

# è¯»å–å¥–åŠ±ç‰¹å¾
for i in range(len(dataset)):
    sample = dataset[i]
    
    state = sample["observation.state"]
    action = sample["action"]
    reward = sample["reward"]  # å¥–åŠ±ç‰¹å¾ï¼ˆå·²å­˜å‚¨çš„å€¼ï¼‰
    done = sample["done"]
    
    print(f"Step {i}: reward={reward.item()}")
```

### ç¤ºä¾‹ 4ï¼šä½¿ç”¨å¥–åŠ±å¤„ç†å™¨

```python
from lerobot.processor.pipeline import RewardProcessorStep, DataProcessorPipeline
from lerobot.processor.core import TransitionKey, EnvTransition

class ShapedRewardProcessor(RewardProcessorStep):
    """å¥–åŠ±å¡‘å½¢å¤„ç†å™¨"""
    
    def reward(self, reward):
        """ä¿®æ”¹å¥–åŠ±å€¼"""
        # ä¾‹å¦‚ï¼šæ·»åŠ å¥–åŠ±å¡‘å½¢
        shaped_reward = reward + 0.1  # æ·»åŠ å°çš„æ­£å¥–åŠ±
        return shaped_reward

# åˆ›å»ºå¤„ç†å™¨ç®¡é“
reward_processor = ShapedRewardProcessor()

# åœ¨ç¯å¢ƒæ­¥éª¤ä¸­ä½¿ç”¨
transition = create_transition(
    observation=obs,
    action=action,
    reward=env_reward,  # ç¯å¢ƒå¥–åŠ±å‡½æ•°çš„å€¼
    done=done
)

# é€šè¿‡å¤„ç†å™¨ä¿®æ”¹å¥–åŠ±
processed_transition = reward_processor(transition)
final_reward = processed_transition[TransitionKey.REWARD]
```

---

## æ€»ç»“

### å¥–åŠ±å‡½æ•°ï¼ˆReward Functionï¼‰

- **å®šä¹‰ä½ç½®**ï¼šç¯å¢ƒä¸­
- **è®¡ç®—æ—¶æœº**ï¼šè¿è¡Œæ—¶ï¼ˆæ¯æ¬¡ `step()`ï¼‰
- **è¾“å…¥**ï¼š`(state, action, next_state)`
- **è¾“å‡º**ï¼šæ ‡é‡å¥–åŠ±å€¼
- **ç”¨é€”**ï¼šåœ¨çº¿å¼ºåŒ–å­¦ä¹ è®­ç»ƒ

### å¥–åŠ±ç‰¹å¾ï¼ˆReward Featureï¼‰

- **å®šä¹‰ä½ç½®**ï¼šæ•°æ®é›†ä¸­
- **å­˜å‚¨æ—¶æœº**ï¼šè®°å½•æ•°æ®æ—¶
- **è¾“å…¥**ï¼šæ— ï¼ˆå·²ç»æ˜¯å€¼ï¼‰
- **è¾“å‡º**ï¼šæ ‡é‡å¥–åŠ±å€¼ï¼ˆå·²å­˜å‚¨ï¼‰
- **ç”¨é€”**ï¼šç¦»çº¿è®­ç»ƒã€æ•°æ®åˆ†æ

### å…³é”®åŒºåˆ«

1. **å¥–åŠ±å‡½æ•°**æ˜¯è®¡ç®—å¥–åŠ±çš„**å‡½æ•°**ï¼Œåœ¨ç¯å¢ƒä¸­å®šä¹‰
2. **å¥–åŠ±ç‰¹å¾**æ˜¯å­˜å‚¨å¥–åŠ±çš„**æ•°æ®**ï¼Œåœ¨æ•°æ®é›†ä¸­å­˜å‚¨
3. **å¥–åŠ±å‡½æ•°**çš„å€¼ä¼šè¢«å­˜å‚¨ä¸º**å¥–åŠ±ç‰¹å¾**

### æœ€ä½³å®è·µ

1. **å®šä¹‰å¥–åŠ±å‡½æ•°**ï¼šåœ¨ç¯å¢ƒçš„ `step()` æ–¹æ³•ä¸­è®¡ç®—å¥–åŠ±
2. **ä½¿ç”¨å¥–åŠ±å¤„ç†å™¨**ï¼šå¯ä»¥åœ¨å¥–åŠ±å‡½æ•°åä¿®æ”¹å¥–åŠ±å€¼
3. **å­˜å‚¨å¥–åŠ±ç‰¹å¾**ï¼šè®°å½•æ•°æ®æ—¶ï¼Œå°†å¥–åŠ±å‡½æ•°çš„å€¼å­˜å‚¨ä¸ºå¥–åŠ±ç‰¹å¾
4. **ä½¿ç”¨å¥–åŠ±ç‰¹å¾**ï¼šç¦»çº¿è®­ç»ƒæ—¶ï¼Œä»æ•°æ®é›†ä¸­è¯»å–å¥–åŠ±ç‰¹å¾

---

è¿™äº›æ¦‚å¿µå¯¹äºç†è§£å¼ºåŒ–å­¦ä¹ è®­ç»ƒæµç¨‹éå¸¸é‡è¦ï¼

